{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Numpy Implementation for GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here GCN is implemented according to the paper \"Semi-Supervised Classification with Graph Convolutional Networks\"  Thomas N. Kipf,Max Welling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GCNPNG.PNG\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1). Adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [1, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0]],\n",
    "    dtype=float\n",
    ").T\n",
    "\n",
    "display(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are the same as the adjacency matrix above, this will be helpful for the our following adjacency matrix construction\n"
     ]
    }
   ],
   "source": [
    "def adjacencyBuild(n, neg, num):\n",
    "    \"\"\"n: current node, neg: neighbour node, num: number of the nodes\"\"\"\n",
    "    \n",
    "    if len(n) != len(neg):\n",
    "        print(\"error\")\n",
    "        return \n",
    "    N = len(n)\n",
    "    A = np.zeros((num, num))\n",
    "    for i in range(N):\n",
    "        A[n[i]-1, neg[i]-1] = 1\n",
    "    return A.T\n",
    "        \n",
    "node = [1,2,2,3,4,4,5]\n",
    "neig = [3,1,3,5,3,5,1]\n",
    "\n",
    "testA = adjacencyBuild(node, neig, 5)\n",
    "display(testA)\n",
    "print(\"There are the same as the adjacency matrix above, this will be helpful for the our following adjacency matrix construction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2). Define 2-channel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  1],\n",
       "       [ 2, -1],\n",
       "       [ 1, -1],\n",
       "       [ 3, -3],\n",
       "       [ 5,  0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = np.array([\n",
    "    [6, 1],\n",
    "    [2, -1],\n",
    "    [1, -1],\n",
    "    [3, -3],\n",
    "    [5, 0],\n",
    "])\n",
    "\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X(features):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6,  1],\n",
       "       [ 2, -1],\n",
       "       [ 1, -1],\n",
       "       [ 3, -3],\n",
       "       [ 5,  0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AX:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7., -1.],\n",
       "       [ 0.,  0.],\n",
       "       [11., -3.],\n",
       "       [ 0.,  0.],\n",
       "       [ 4., -4.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"A:\")\n",
    "display(A)\n",
    "print(\"X(features):\")\n",
    "display(features)\n",
    "print(\"AX:\")\n",
    "display(A.dot(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph convolutional layer represents each node as the aggregation of its adjacent nodes.\n",
    "As we can see, the representation of each node (each row) is the sum of the features of its adjacent nodes (the adjacent nodes that the node points to).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3). Add self loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "I = np.eye(A.shape[0])\n",
    "display(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_hat = A + I\n",
    "display(A_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A*X:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13.,  0.],\n",
       "       [ 2., -1.],\n",
       "       [12., -4.],\n",
       "       [ 3., -3.],\n",
       "       [ 9., -4.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"A*X:\")\n",
    "display(A_hat.dot(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0., 0.],\n",
       "       [0., 3., 0., 0., 0.],\n",
       "       [0., 0., 2., 0., 0.],\n",
       "       [0., 0., 0., 3., 0.],\n",
       "       [0., 0., 0., 0., 2.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D = np.diag(np.array(np.sum(A_hat, axis=0)))\n",
    "display(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.33333333, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.5       , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.33333333, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.5       ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.70710678, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.57735027, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.70710678]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D_half = np.diag(np.array(np.power(np.sum(A_hat, axis=0),-0.5)))\n",
    "display(D_half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4). normalization\n",
    "\n",
    "$\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}X$\n",
    "\n",
    "$\\hat{D}^{-1}\\hat{A}X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization1:\n",
      "\n",
      "[[ 6.31649658  0.09175171]\n",
      " [ 0.66666667 -0.33333333]\n",
      " [ 5.54124145 -1.63299316]\n",
      " [ 1.         -1.        ]\n",
      " [ 4.22474487 -1.72474487]]\n",
      "normalization2:\n",
      "\n",
      "[[ 6.5         0.        ]\n",
      " [ 0.66666667 -0.33333333]\n",
      " [ 6.         -2.        ]\n",
      " [ 1.         -1.        ]\n",
      " [ 4.5        -2.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"normalization1:\\n\")#np.linalg.matrix_power(D, -1)\n",
    "print(np.dot(np.dot(np.dot(D_half,A_hat),D_half), features))\n",
    "print(\"normalization2:\\n\")#np.linalg.matrix_power(D, -1)\n",
    "print(np.dot(np.dot(np.linalg.inv(D),A_hat), features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5). Add weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([\n",
    "             [1, -0.5],\n",
    "             [-0.6, 1]\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add weights:\n",
      "\n",
      "[[ 3.8  -2.25]\n",
      " [ 0.    0.  ]\n",
      " [ 6.4  -4.25]\n",
      " [ 0.    0.  ]\n",
      " [ 3.2  -3.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"add weights:\\n\")\n",
    "print(np.dot(np.dot(np.dot(np.linalg.inv(D),A), features),W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add activation function(tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9989996 , -0.97802611],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.99999448, -0.99959315],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.9966824 , -0.99505475]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(np.tanh(np.dot(np.dot(np.dot(np.linalg.inv(D),A), features),W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97811873, 0.09534946],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.9983412 , 0.01406363],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.96083428, 0.04742587]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigmoid = lambda x:1/(1+np.exp(-x))\n",
    "display(sigmoid(np.dot(np.dot(np.dot(np.linalg.inv(D),A), features),W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Application in Cora Dataset\n",
    "\n",
    "Here, a semi-supervised task about Cora Dataset is implemented step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "#do a little modified compared to before\n",
    "def adjacencyBuild(n, neg, num):\n",
    "    \"\"\"n: current node, neg: neighbour node, num: number of the nodes\"\"\"\n",
    "    if len(n) != len(neg):\n",
    "        print(\"error\")\n",
    "        return \n",
    "    N = len(n)\n",
    "    A = np.zeros((num, num))\n",
    "    for i in range(N):\n",
    "        A[n[i], neg[i]] = 1\n",
    "        A[neg[i], n[i]] = 1\n",
    "    return A\n",
    "\n",
    "\n",
    "#define a accuracy calculation, (correct prediction)/ total\n",
    "def acc_calc(true, pred):\n",
    "    correct = pred.eq(true).sum().item()\n",
    "    total = test_mask.sum().item()\n",
    "    return (correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1). data prepocessing(you can ignore this part, but do figure out what the data format is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.Tensor(np.load(\"data/train_mask.npy\")).bool()\n",
    "test_mask = torch.Tensor(np.load(\"data/test_mask.npy\")).bool()\n",
    "features = torch.Tensor(np.load(\"data/features.npy\"))\n",
    "label_list = torch.Tensor(np.load(\"data/label_list.npy\"))\n",
    "edge_index = np.load(\"data/edge_index.npy\")\n",
    "feat_dim = features.shape[1]\n",
    "num_class = int(label_list.max().data.numpy() + 1) #7\n",
    "\n",
    "node_num = features.shape[0]\n",
    "A = adjacencyBuild(edge_index[0,:],edge_index[1,:], node_num)\n",
    "A = A + np.eye(A.shape[0])\n",
    "A = torch.Tensor(A)\n",
    "D = torch.diag(pow(torch.sum(A, axis=0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** data info******************************\n",
      "number of node : 2708\n",
      "number of edges : 5278\n",
      "number of classes : 7\n",
      "channel of node features : 1433\n",
      "shape of feat_Matrix : torch.Size([2708, 1433])\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"{} data info{}\".format(\"*\"*30, \"*\"*30))\n",
    "print(\"number of node : {}\".format(node_num))\n",
    "print(\"number of edges : {}\".format(round(edge_index.shape[1]/2)))\n",
    "print(\"number of classes : {}\".format(num_class))\n",
    "print(\"channel of node features : {}\".format(feat_dim))\n",
    "print(\"shape of feat_Matrix : {}\".format(features.shape))\n",
    "print(\"{}***********{}\".format(\"*\"*30, \"*\"*30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2). This part show you some basic principles, you can also jump this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = lambda x:np.maximum(0,x)\n",
    "sigmoid = lambda x:1/(1+np.exp(-x))\n",
    "\n",
    "def gcn_layer(A, D, X, W, activation = 'relu'):\n",
    "    if activation == 'relu':\n",
    "        return relu(np.dot(np.dot(np.dot(np.linalg.inv(D),A),X),W))\n",
    "    elif activation == 'sigmoid':\n",
    "        return sigmoid(np.dot(np.dot(np.dot(np.linalg.inv(D),A),X),W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_N = np.random.normal(loc=0, scale=1, size=(feat_dim, node_num))\n",
    "W_W = np.random.normal(loc=0, size=(node_num, num_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = gcn_layer(A, D, features, W_N, activation = 'relu')\n",
    "output = gcn_layer(A, D, hidden, W_W, activation = 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.nn.Softmax(dim=1)(torch.Tensor(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 7])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 5,  ..., 5, 5, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result.size())\n",
    "display(result)\n",
    "display(torch.argmax(result,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0668)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(result, label_list.long())\n",
    "display(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3).Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_layer(A, D, X, W, activation = 'relu'):\n",
    "    if activation == 'relu':\n",
    "        return torch.relu(torch.mm(torch.mm(torch.mm(D,A),X),W))\n",
    "    elif activation == 'sigmoid':\n",
    "        return torch.sigmoid(torch.mm(torch.mm(torch.mm(D,A),X),W))\n",
    "    else:\n",
    "        return torch.mm(torch.mm(torch.mm(torch.mm(D,A),D),X),W)\n",
    "    \n",
    "def two_layer_gcn(A, D, X, W1, W2):\n",
    "    hidden = gcn_layer(A, D, X, W1, activation = None) # we do not add any activation\n",
    "    output = gcn_layer(A, D, hidden, W2, activation = None)\n",
    "    return torch.nn.Softmax(dim=1)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_N = np.random.normal(loc=0, scale=1, size=(feat_dim, 50))\n",
    "W_W = np.random.normal(loc=0, size=(50, num_class))\n",
    "W_N = torch.autograd.Variable(torch.Tensor(W_N), requires_grad=True)\n",
    "W_W = torch.autograd.Variable(torch.Tensor(W_W), requires_grad=True)\n",
    "\n",
    "A = torch.Tensor(A)\n",
    "D = torch.Tensor(D)\n",
    "features = torch.Tensor(features)\n",
    "\n",
    "learning_rate = 1# the smaller it would be better, but after that, you need to increase the size of epoches\n",
    "losses = []\n",
    "epoches = 200\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.zeros(node_num, dtype=torch.bool)\n",
    "train_mask[:node_num - 1000] = 1                  # 1700 training\n",
    "test_mask = torch.zeros(node_num, dtype=torch.bool)\n",
    "test_mask[node_num - 500:] = 1                    # 500 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if we do not train, the accuracy is : 0.142000\n"
     ]
    }
   ],
   "source": [
    "prediction = two_layer_gcn(A, D, features, W_N, W_W)\n",
    "prediction = torch.argmax(prediction,dim=1)\n",
    "correct = float(prediction[test_mask].eq(label_list[test_mask]).sum().item())\n",
    "accuracy = correct / test_mask.sum().item()\n",
    "print('if we do not train, the accuracy is : {:.6f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:1.9543739557266235\n",
      "loss:1.9424790143966675\n",
      "loss:1.9297372102737427\n",
      "loss:1.9173548221588135\n",
      "loss:1.903292179107666\n",
      "loss:1.8872184753417969\n",
      "loss:1.8731129169464111\n",
      "loss:1.8610827922821045\n",
      "loss:1.85047447681427\n",
      "loss:1.841111421585083\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoches):\n",
    "    #hidden = gcn_layer(A_hat, D, feat_Matrix, W_N, activation = 'relu')\n",
    "    #output = gcn_layer(A_hat, D, hidden, W_W, activation = 'relu')\n",
    "    #result = torch.nn.Softmax(dim=1)(output)\n",
    "    \n",
    "    result = two_layer_gcn(A, D, features, W_N, W_W)\n",
    "    loss = loss_fn(result[train_mask], label_list[train_mask].long())\n",
    "    \n",
    "    loss.backward()\n",
    "    W_N.data.add_(-learning_rate*W_N.grad.data)\n",
    "    W_W.data.add_(-learning_rate*W_W.grad.data)\n",
    "    \n",
    "    W_N.grad.data.zero_()\n",
    "    W_W.grad.data.zero_()\n",
    "    losses.append(loss.data.numpy())\n",
    "    if i % 20 == 0: print('loss:{}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training, the accuracy: 0.402000\n"
     ]
    }
   ],
   "source": [
    "prediction = two_layer_gcn(A, D, features, W_N, W_W)\n",
    "prediction = torch.argmax(prediction,dim=1)\n",
    "correct = float(prediction[test_mask].eq(label_list[test_mask]).sum().item())\n",
    "accuracy = correct / test_mask.sum().item()\n",
    "print('after training, the accuracy: {:.6f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3RU5b7G8e+PQCgiHeFI7wKBRAhdQhUQpal4pEoHRcTG1YP9KseGokhvAop0UESlCiJSEwg1IB2jKBEpNpTy3j8yuNBLIJBJ9mTyfNbKItnvzsyTneHJnnf27G3OOUREJHhl8jqAiIikLhW9iEiQU9GLiAQ5Fb2ISJBT0YuIBDkVvYhIkLti0ZvZJDM7ambbkxjPa2bzzWyrmW0ws7CLxvKY2Rwz22VmcWZWx5/hRUTkypKzRz8ZaHGZ8cFArHOuKtAVePuisbeBRc65m4BwIO4ac4qIyDW6YtE751YBP11mlUrAct+6u4CSZlbIzHIBUcBE39ifzrkTKY8sIiJXI7MfbmMLcCew2sxqAiWAosA5IAF418zCgRhgoHPu1yvdYIECBVzJkiX9EE1EJGOIiYn50TlX8FJj/ij6V4C3zSwW2AZsBs4CWYBqwADn3Hozext4EnjmUjdiZn2APgDFixcnOjraD9FERDIGMzuU1FiKj7pxzp1yznV3zkWQOEdfEDgAxAPxzrn1vlXnkFj8Sd3OOOdcpHMusmDBS/5REhGRa5DiovcdWRPq+7IXsMpX/t8D35hZBd9YE2BnSu9PRESuzhWnbsxsOtAQKGBm8cBzJE7L4JwbA1QEpprZORKLvOdF3z4AmOb7Q7Af6O7X9CIickVXLHrnXIcrjK8FyiUxFgtEXls0EZG/O3PmDPHx8Zw+fdrrKJ7Jli0bRYsWJUuWLMn+Hn+8GCsikibi4+O5/vrrKVmyJGbmdZw055zj2LFjxMfHU6pUqWR/n06BICLpxunTp8mfP3+GLHkAMyN//vxX/YxGRS8i6UpGLfkLruXnD6qif/GLF1n7zVqvY4hIEMuZM6fXEa5a0BT9idMnGBMzhrqT6tJpXifiT8V7HUlEJCAETdHnyZaH3Q/u5qn6TzF351zKv1OeF1a+wG9nfvM6mogEIeccgwYNIiwsjCpVqjBz5kwAjhw5QlRUFBEREYSFhfHll19y7tw5unXr9te6w4YNA2Dfvn20aNGC6tWrU79+fXbt2gXA7NmzCQsLIzw8nKioKP+EDbSP6tWru5Q4cPyAaz+rveN5XLE3i7np26a78+fPp+g2RcR7O3fu9DqCu+6665xzzs2ZM8c1bdrUnT171n3//feuWLFi7rvvvnNDhw51L730knPOubNnz7pTp0656Oho17Rp079u4/jx48455xo3buy+/vpr55xz69atc40aNXLOORcWFubi4+P/tu7FLrUdgGiXRKcG5eGVJfOUZFb7Waw6tIqBiwbSYW4HRmwYwVst3iLyRh3WLxIMHl70MLHfx/r1NiMKR/BWi7eSte7q1avp0KEDISEhFCpUiAYNGrBx40Zq1KhBjx49OHPmDG3btiUiIoLSpUuzf/9+BgwYwO23306zZs345ZdfWLNmDe3bt//rNv/44w8A6tWrR7du3bjnnnu48847U/xzBc3UzaVElYgiunc041uNZ89Pe6g5viZ9P+7LT79f7qzLIiJXlrgT/f9FRUWxatUqihQpQpcuXZg6dSp58+Zly5YtNGzYkJEjR9KrVy/Onz9Pnjx5iI2N/esjLi7xkh1jxozhpZde4ptvviEiIoJjx46lPGygfaR06uZSTp4+6R5Z9IgLeSHEFXytoJsSO0XTOSLpTCBN3cydO9c1a9bMnT171h09etQVL17cHTlyxB08eNCdOXPGOefcsGHD3MCBA11CQoI7efKkc865zZs3u/DwcOecc3Xq1HGzZs1yzjl3/vx5Fxsb65xzbu/evX/dX0REhNu8efPfMlzt1E1Q79FfLFfWXLzZ/E2i+0RTJl8Z7vvwPhpNacTOBJ1nTUSuXrt27ahatSrh4eE0btyY1157jcKFC7Ny5UoiIiK4+eabmTt3LgMHDuTbb7+lYcOGRERE0K1bN15++WUApk2bxsSJEwkPD6dy5cp89NFHAAwaNIgqVaoQFhZGVFQU4eHhKcpqLomnH16KjIx0qXk++vPuPBM3TeSJZU/w858/81idx3i2wbPkyJIj1e5TRFIuLi6OihUreh3Dc5faDmYW45y75IuQGWaP/mKZLBO9q/dm94O76Vy1M69+9SpVR1dl5cGVXkcTEfG7DFn0FxS8riDvtnmXz7t+DkCjKY3o+3FfTp4+6XEyERH/ydBFf0GjUo3Yev9WHqvzGBM2T6DyqMos/Hqh17FERPxCRe+TI0sOhjYbytqea8mbPS+tprei49yOJPya4HU0EblIIL6umJau5edX0f9DzSI1iekTwwsNX2DOzjlUHFmR6dumZ/gHl0ggyJYtG8eOHcuw/x+d73z02bJlu6rvy5BH3STXjqM76LmgJ+u/Xc9dFe9i9O2jKXidLlwu4hVdYSrpK0xd7qgbFf0VnDt/jqFrhvLsymfJky0P4+4YR5ub2ngdS0Tkb3R4ZQqEZArhiVueILp3NDdefyNtZ7blvg/v48TpE15HExFJFhV9MlUpVIX1vdbzTNQzTNs6jSqjq7Bs/zKvY4mIXJGK/iqEhoTyv43+lzU915AzNCe3vncr/T/pz69//up1NBGRJKnor0HNIjXZ1GcTj9R+hNHRo4kYG8Gab9Z4HUtE5JJU9Ncoe5bsvNn8TVbct4Kz589S/936/GfZf/jj7B9eRxMR+ZsrFr2ZTTKzo2a2PYnxvGY238y2mtkGMwv7x3iImW02s6B8q2mDkg3Y2m8rPSJ68MpXr1BzQk22fL/F61giIn9Jzh79ZKDFZcYHA7HOuapAV+Dtf4wPBOKuKV06cX3W6xnfejwfd/iYH375gRrja/Dyly9z9vxZr6OJiFy56J1zq4DLXZKpErDct+4uoKSZFQIws6LA7cCElEcNfHeUv4PtD2ynzU1tGPz5YKLejWLPsT1exxKRDM4fc/RbgDsBzKwmUAIo6ht7C/gf4Lwf7iddKJCjALPunsW0O6cR92McEWMjGLVxVIZ9y7aIeM8fRf8KkNfMYoEBwGbgrJndARx1zsUk50bMrI+ZRZtZdEJC+j6RmJnRsUpHtt+/nfrF69P/0/40f7858afivY4mIhlQsk6BYGYlgYXOubArrGfAAaAq8B+gC3AWyAbkAuY55zpf6f4C6RQIKeWcY2zMWB5b8hhZMmVhRMsRdKrSicRNJSLiH6l6CgQzy2Nmob4vewGrnHOnnHP/cc4Vdc6VBO4FPk9OyQcbM6NfZD+29NtC5Rsq02V+F9rPbq/TH4tImknO4ZXTgbVABTOLN7OeZtbPzPr5VqkI7DCzXcBtJB5lI/9QNl9ZVnVbxatNX+Xjrz8mbHQYC3Yv8DqWiGQAOnulB7b+sJUu87uw9YetdI/ozlst3iJX1lxexxKRdExnrwwwVQtVZWPvjQy+ZTBTtkzRhclFJFWp6D0SGhLKkCZDWN19NaEhoTSa0ohHFj3C72d+9zqaiAQZFb3H6hSrw+a+m+lfoz9vrX+LauOqsfHbjV7HEpEgoqIPANeFXseIliNY0nkJP//xM3Um1uH5lc9z5twZr6OJSBBQ0QeQW8vcyvYHttOxSkde+OIF6kysw86EnV7HEpF0TkUfYPJky8PUdlOZe89cDp08RLWx1XhjzRucO3/O62gikk6p6APUnRXvZPv922letjmPL32c+u/WJy4hqE8CKiKpREUfwArlLMSH//6Q99u9z+5ju4kYG6HTH4vIVVPRBzgzo1PVTux8YCetK7Rm8OeDqTWhli5uIiLJpqJPJwrlLMTs9rOZ034O8afiiRwfybMrntWlC0XkilT06cxdle5i5wM76RDWgRdXvUj1cdXZ8O0Gr2OJSABT0adD+XPkZ2q7qSzssJATp09QZ2IdBi0ZpHfVisglqejTsdvL386OB3bQ8+aeDF07lPAx4Xx56EuvY4lIgFHRp3O5s+VmXKtxLOuyjDPnzxA1OYoBnw7glz9/8TqaiAQIFX2QaFK6Cdvu38ZDNR9i5MaRhI0KY+m+pV7HEpEAoKIPIjlDc/L2bW+zqvsqsmbOSrP3m9Hzo56cOH3C62gi4iEVfRC6pfgtxPaN5Yl6TzB5y2Qqj6rMwq8Xeh1LRDyiog9S2bNk55Wmr7Cu5zryZc9Hq+mt6DyvM8d+O+Z1NBFJYyr6IFejSA1i+sTwXIPnmLljJpVGVWLOzjlexxKRNKSizwBCQ0J5vuHzRPeOpmiuorSf3Z72s9vzwy8/eB1NRNKAij4DCS8czvpe6/lv4/+yYPcCKo2qxLSt0wjEC8SLiP+o6DOYzJky85/6/yG2byzl85en8/zOtJ7Rmm9Pfet1NBFJJSr6DKpiwYqs7r6aN5u9yfL9y6k8qjKTNk/S3r1IELpi0ZvZJDM7ambbkxjPa2bzzWyrmW0wszDf8mJmtsLM4sxsh5kN9Hd4SZmQTCE8UucRtt6/lYjCEfRc0JPm7zfn0IlDXkcTET9Kzh79ZKDFZcYHA7HOuapAV+Bt3/KzwGPOuYpAbaC/mVVKQVZJJWXzleXz+z5nVMtRrI1fS9joMEZtHMV5d97raCLiB1cseufcKuCny6xSCVjuW3cXUNLMCjnnjjjnNvmW/wzEAUVSHllSQybLxP017mf7/dupW6wu/T/tT+Mpjdn7016vo4lICvljjn4LcCeAmdUESgBFL17BzEoCNwPr/XB/kopK5CnBok6LmNh6IrHfx1J1dFWGrR2mi5OLpGP+KPpXgLxmFgsMADaTOG0DgJnlBOYCDzvnTiV1I2bWx8yizSw6ISHBD7HkWpkZPW7uwY4HdtCkdBMeXfIojaY0Yv/x/V5HE5FrkOKid86dcs51d85FkDhHXxA4AGBmWUgs+WnOuXlXuJ1xzrlI51xkwYIFUxpL/KBIriIsuHcBU9pOYcsPW6g6uirjY8bryByRdCbFRW9mecws1PdlL2CVc+6UmRkwEYhzzr2Z0vsRb5gZXcO7su3+bdQuWps+C/twx/Q7OPLzEa+jiUgyJefwyunAWqCCmcWbWU8z62dm/XyrVAR2mNku4DbgwmGU9YAuQGMzi/V9tEyFn0HSQPHcxVnSZQnDWwxnxYEVhI0OY9aOWV7HEpFksEB8Gh4ZGemio6O9jiFJ2P3jbrp+2JUN326gQ1gHRrQcQb7s+byOJZKhmVmMcy7yUmN6Z6xctQoFKvBVj694sdGLzN45myqjq7B472KvY4lIElT0ck0yZ8rM01FPs77XevJky0OLaS24f+H9ulatSABS0UuKVPtXNWL6xPBYnccYGzOWiDERfHX4K69jichFVPSSYtkyZ2Nos6Gs7LaSc+4cUZOjeGr5U5w5d8braCKCil78KKpEFFv7beW+8Pv47+r/EjU5igPHD3gdSyTDU9GLX12f9XomtZnEjLtmEJcQR8TYCKZvm+51LJEMTUUvqeLfYf8mtl8sYTeE0XFeR7p/1F0v1Ip4REUvqaZknpJ80e0Lno16lqlbplJtbDVivovxOpZIhqOil1SVOVNmXmj0AivuW8HvZ3+nzsQ6DF0zVOe6F0lDKnpJE1ElotjSbwutKrRi0NJB3DbtNr7/5XuvY4lkCCp6STP5sudjTvs5jL1jLF8e+pKIMRGsOLDC61giQU9FL2nKzOhTvQ8be28kb/a8NH2vKUNWDdFUjkgqUtGLJyrfUJmNvTdyb9i9PL3iaVpOa8mPv/3odSyRoKSiF8/kDM3J++3eZ8ztY1h5cCU3j72ZNd+s8TqWSNBR0YunzIy+kX1Z03MNoSGhNJjcgGFrh+kqViJ+pKKXgFDtX9XY1GcTrcq34tElj9J5fmd+O/Ob17FEgoKKXgJG7my5mXvPXIY0HsL0bdOpN6keB08c9DqWSLqnopeAYmYMrj+YhR0XcuD4ASLHRbJ8/3KvY4mkayp6CUgty7VkY++NFMpZiGbvN+ONNW9o3l7kGqnoJWCVy1+OdT3X0e6mdjy+9HE6zeukeXuRa6Cil4B2fdbrmd1+NkMaD2HG9hnUnVhX8/YiV0lFLwHvwrz9Jx0/4eCJg9SaUIt18eu8jiWSbqjoJd24rdxtrOu1jpyhOWk4uSEzts/wOpJIuqCil3TlpgI3sb7XemoUqUGHuR343y/+Vy/SilzBFYvezCaZ2VEz257EeF4zm29mW81sg5mFXTTWwsx2m9leM3vSn8El4yqQowDLuiyja3hXnlv5HJ3nd+b02dNexxIJWMnZo58MtLjM+GAg1jlXFegKvA1gZiHASOA2oBLQwcwqpSitiE/WzFmZ3GYyQxoP4YNtH9B4SmOO/nrU61giAemKRe+cWwX8dJlVKgHLfevuAkqaWSGgJrDXObffOfcnMANok/LIIokuvEg7u/1sNn+/mdoTarPrx11exxIJOP6Yo98C3AlgZjWBEkBRoAjwzUXrxfuWifjV3ZXuZuV9K/n1zK/UmViHlQdXeh1JJKD4o+hfAfKaWSwwANgMnAXsEusm+aqZmfUxs2gzi05ISPBDLMlIahWtxbqe6/hXzn/R7L1mTN0y1etIIgEjxUXvnDvlnOvunIsgcY6+IHCAxD34YhetWhT47jK3M845F+mciyxYsGBKY0kGVCpvKb7q8RW3FL+F+z68j+dXPq8jckTwQ9GbWR4zC/V92QtY5Zw7BWwEyplZKd/4vcCClN6fyOXkzZ6XRZ0X0S2iGy988QJdP+zKH2f/8DqWiKcyX2kFM5sONAQKmFk88ByQBcA5NwaoCEw1s3PATqCnb+ysmT0ILAZCgEnOuR2p8UOIXCw0JJRJrSdRJm8ZnlnxDIdPHmb+v+eTL3s+r6OJeMIC8altZGSki46O9jqGBIEPtn1A94+6UypPKT7p+All8pXxOpJIqjCzGOdc5KXG9M5YCWodq3RkWZdlJPyWQO2JtVn7zVqvI4mkORW9BL36Jeqztuda8mTLQ6MpjZi9Y7bXkUTSlIpeMoTy+cuztudaIm+M5J459/Dq6ld1RI5kGCp6yTAK5CjAsq7LuDfsXp5c/iR9F/blzLkzXscSSXVXPOpGJJhky5yNaXdOo3Se0vx39X85dPIQs+6eRe5sub2OJpJqtEcvGU4my8SQJkOY2Hoinx/4nFvevYXDJw97HUsk1ajoJcPqcXMPPuv0GYdPHqbWhFrEfBfjdSSRVKGilwytaemmrOmxhqwhWYmaHMWC3XrztgQfFb1keJVvqMy6XuuoXLAybWe0Zfj64V5HEvErFb0IUDhnYVZ2W0mbm9owcNFAHvrsIc6dP+d1LBG/UNGL+OTIkoM57efwaO1HeWfDO7Sb2Y5f//zV61giKaaiF7lISKYQ3mj+BiNbjuSTPZ8QNTmKIz8f8TqWSIqo6EUu4YEaD7Dg3gXs/nE3tSbUYsv3W7yOJHLNVPQiSbi9/O2s7rGa8+48dSfVZdaOWV5HErkmKnqRy4goHEF0n2giCkfw7zn/ZvDywXqRVtIdFb3IFRTOWZjPu35O72q9eXn1y7Se0ZoTp094HUsk2VT0IsmQNXNWxrUax+jbR7Nk3xJqTahFXEKc17FEkkVFL3IV+kX2Y3nX5Rz//Ti1JtTi490fex1J5IpU9CJXKapEFDF9YiiXvxxtZrThpVUvcd6d9zqWSJJU9CLXoFjuYqzuvpqOVTryzIpnaDujLcd/P+51LJFLUtGLXKPsWbLzXrv3GN5iOJ/t/YzI8ZHEfh/rdSyR/0dFL5ICZsaAWgP4otsX/HH2D+pMrMPk2MlexxL5GxW9iB/ULVaXTX03UbdYXbp/1J0+H/fh9NnTXscSAVT0In5zw3U3sKTzEgbfMpjxm8ZTb1I9Dhw/4HUskeQVvZlNMrOjZrY9ifHcZvaxmW0xsx1m1v2isdd8y+LMbLiZmb/CiwSakEwhDGkyhAX3LmDfT/uoPq46n+751OtYksEld49+MtDiMuP9gZ3OuXCgIfCGmYWaWV2gHlAVCANqAA2uOa1IOtGqQiti+sRQPHdxbv/gdp5d8axOnSCeSVbRO+dWAT9dbhXget/eek7fumd9y7MBoUBWIAvwQ0oCi6QXZfKVYW3PtXSL6MaLq17k1vdu1SmPxRP+mqMfAVQEvgO2AQOdc+edc2uBFcAR38di59wl3zduZn3MLNrMohMSEvwUS8Rb2bNkZ1LrSbzb5l3Wf7ue8DHhLN672OtYksH4q+ibA7HAjUAEMMLMcplZWRL/ABQFigCNzSzqUjfgnBvnnIt0zkUWLFjQT7FEvGdmdIvoRnTvaArlLESLaS14ctmTnDl3xutokkH4q+i7A/Ncor3AAeAmoB2wzjn3i3PuF+AzoLaf7lMkXalYsCIbem2gb/W+vPrVqzSY3IBDJw55HUsyAH8V/WGgCYCZFQIqAPt9yxuYWWYzy0LiC7E65Z9kWNmzZGfMHWOYefdMdiTsIGJsBB/u+tDrWBLkknt45XRgLVDBzOLNrKeZ9TOzfr5VXgTqmtk2YDnwhHPuR2AOsI/EefstwBbnnE73JxnePZXvYXPfzZTNV5Z2M9sx4NMBeoOVpBpzznmd4f+JjIx00dHRXscQSXV/nvuTJ5c9ybB1w7i58M3MuHsG5fOX9zqWpENmFuOci7zUmN4ZK+Kh0JBQ3mz+JgvuXcChk4e4eezNjIsZRyDugEn6paIXCQCtKrRia7+t1C1Wl74L+9J2ZlsSftVhxuIfKnqRAFEkVxEWd17MsObDWLR3EVVGV+GzPZ95HUuCgIpeJIBkskw8XPthNvbeSMHrCtLyg5YM+HQAv5/53etoko6p6EUCUNVCVdnYeyMP13qYERtH6KImkiIqepEAlS1zNoa1GMbizos5/vtxao6vydA1Q3V9WrlqKnqRANesTDO23r+VO8rfwaClg2g8pTH7j+/3OpakIyp6kXSgQI4CzL1nLpNaT2Lz95upOroqozaO0t69JIuKXiSdMDO639yd7fdvp17xevT/tD+3vncrB08c9DqaBDgVvUg6Uyx3MRZ1WsS4O8ax8duNVBldRW+ykstS0YukQ2ZG7+q92Xb/NmoVqUXfhX1p/n5zDp887HU0CUAqepF0rESeEiztspTRt49mzTdrCBsVxsRNE7V3L3+johdJ58yMfpH92Hb/NqrfWJ1eH/fitmm3ae5e/qKiFwkSpfKWYnnX5Yy4bQRfffMVlUdV5q11b+mi5KKiFwkmmSwT/Wv2Z8cDO2hYsiGPLH6EOhPrsPWHrV5HEw+p6EWCUPHcxVnYYSHT75rOwRMHqT6uOk8tf0oXN8mgVPQiQcrMuDfsXuL6x9G5amf+u/q/hI8J54uDX3gdTdKYil4kyOXPkZ9327zL0i5LOXPuDA2nNKTPx304cfqE19EkjajoRTKIpqWbsv2B7QyqO4iJmydScWRFPtj2gQ7FzABU9CIZSI4sOXjt1tfY0GsDRXMVpdO8TjSZ2oS4hDivo0kqUtGLZEDVb6zOup7rGH37aDZ/v5nwMeEMXj6YX//81etokgpU9CIZVEimEPpF9mP3g7vpWKUjL69+mUqjKvHRro80nRNkVPQiGdwN193A5LaTWdVtFbmy5qLtzLa0ntGaA8cPeB1N/OSKRW9mk8zsqJltT2I8t5l9bGZbzGyHmXW/aKy4mS0xszgz22lmJf0XXUT8qX6J+mzqs4mhtw5l5cGVVBpViZdWvcQfZ//wOpqkUHL26CcDLS4z3h/Y6ZwLBxoCb5hZqG9sKvC6c64iUBM4eu1RRSS1ZQnJwmN1HyOufxytyrfimRXPUHVMVZbsW+J1NEmBKxa9c24V8NPlVgGuNzMDcvrWPWtmlYDMzrmlvtv5xTn3mx8yi0gqK5qrKLPaz2Jx58Wcd+dp/n5z2s5oq0sYplP+mKMfAVQEvgO2AQOdc+eB8sAJM5tnZpvN7HUzC/HD/YlIGmlWphnb79/Oy01eZtn+ZVQaWYmnP39aR+ekM/4o+uZALHAjEAGMMLNcQGagPvA4UAMoDXRL6kbMrI+ZRZtZdEJCgh9iiYg/ZM2clSdveZKvB3xN+8rtGfLlECqMqMD0bdN1dE464Y+i7w7Mc4n2AgeAm4B4YLNzbr9z7izwIVAtqRtxzo1zzkU65yILFizoh1gi4k83Xn8j77V7j9XdV1MoZyE6zutI1OQoYr+P9TqaXIE/iv4w0ATAzAoBFYD9wEYgr5ldaO3GwE4/3J+IeKhe8Xps6LWB8a3Gs+vHXVQfV51+C/vx428/eh1NkpCcwyunA2uBCmYWb2Y9zayfmfXzrfIiUNfMtgHLgSeccz86586ROG2z3DdmwPjU+TFEJC2FZAqhV7Ve7BmwhwE1BzBh0wTKvVOOYWuH8ee5P72OJ/9ggTjHFhkZ6aKjo72OISLJtOPoDh5b8hiL9y2mbL6yvH7r67Sp0IbEg/EkLZhZjHMu8lJjemesiKRY5Rsqs6jzIj7r9BmhIaG0m9mOxlMbs/nIZq+jCSp6EfGjFmVbsKXfFka2HMn2o9upPq463T/qznc/f+d1tAxNRS8ifpU5U2YeqPEAewbs4bE6jzFt6zTKv1OeF794kd/O6D2TXlDRi0iqyJMtD683e524/nG0KNuCZ1c+S4URFXh/6/ucd+e9jpehqOhFJFWVyVeGOffMYVW3VRS6rhBd5neh9oTarD682utoGYaKXkTSRP0S9dnQewNT2k7h25+/pf679Wk3sx27ftzldbSgp6IXkTSTyTLRNbwrXz/4NS82epHl+5cTNiqMvh/35cjPR7yOF7RU9CKS5q4LvY6no55m30P76F+jP+/GvkvZd8ry9OdPc+qPU17HCzoqehHxTMHrCvL2bW8T1z+O1hVaM+TLIZQZXobh64frHbZ+pKIXEc+VyVeG6XdNZ2PvjVQtVJWBiwZy04ibmL5tuo7Q8QMVvYgEjMgbI1nWZRmLOi0iV9ZcdJzXkZrja7J8/3Kvo6VrKnoRCShmRvOyzdnUdxNT204l4bcEmr7XlObvNyfmuxiv46VLKnoRCUiZLBNdwruw+8HdvNHsDaK/iyZyfCR3zbqL7ajbHoEAAAoTSURBVEe3ex0vXVHRi0hAy5Y5G4/WeZT9D+3n+QbPs3TfUqqOrkqneZ3Yc2yP1/HSBRW9iKQLubPl5rmGz3Fg4AH+p97/MD9uPhVHVqTXgl4cOnHI63gBTUUvIulK/hz5eaXpK+wfuJ8Haz7Ie1vfo9w75Xjw0wf1pqskqOhFJF0qnLMwb7V4i70D9tI9ojtjY8ZSenhpHl38qAr/H1T0IpKuFctdjLGtxrKr/y7+XfnfDF8/nFJvl+Khzx4i/lS81/ECgopeRIJCmXxlmNx2Mrsf3E3nqp0ZHT2aMsPL8MAnD3D45GGv43lKRS8iQaVMvjJMaD2BPQP20D2iOxM2TaDs8LL0+bgPB44f8DqeJ1T0IhKUSuYpyZg7xrDvoX30qd6HKVumUO6dcnT/qHuGOyxTRS8iQa1Y7mKMaDmC/Q8lHqUzY/sMbhp5E13md2Fnwk6v46UJFb2IZAhFchXhrRZvcWDgAR6t/Sjz4uZReVRl2s5oy/r49V7HS1UqehHJUArnLMzrzV7n0MOHeK7Bc6w6tIraE2vTeEpjlu5binPO64h+l6yiN7NJZnbUzC55ggkzy21mH5vZFjPbYWbd/zGey8y+NbMR/ggtIpJSBXIU4PmGz3P4kcO80ewNdh/bTbP3m1FjfA3m7pwbVKdHTu4e/WSgxWXG+wM7nXPhQEPgDTMLvWj8ReCLawkoIpKacobm/OtcOuNbjefkHye5e/bdVBpZiXc3vxsUF0BJVtE751YBP11uFeB6MzMgp2/dswBmVh0oBCxJWVQRkdSTNXNWelXrxa7+u5h590yyZ8lOjwU9KPV2KV776jVOnD7hdcRr5q85+hFAReA7YBsw0Dl33swyAW8Ag/x0PyIiqSokUwj3VL6HTX02sajTIioVrMQTy56g2LBiPLr40XT55it/FX1zIBa4EYgARphZLuAB4FPn3DdXugEz62Nm0WYWnZCQ4KdYIiLX5sIFUJZ2WcrmvptpU6ENw9cPp/Tbpek0rxObj2z2OmKyWXJfYTazksBC51zYJcY+AV5xzn3p+/pz4ElgIFAfOE/ilE4oMMo59+Tl7isyMtJFR0cn/6cQEUkDh08eZvj64YyLGcfPf/5Mk1JNeLzu4zQv05zEmWvvmFmMcy7yUmP+2qM/DDTx3VkhoAKw3znXyTlX3DlXEngcmHqlkhcRCVTFcxdnaLOhfPPIN7zW9DV2/biL26bdRtUxVZkSOyVgX7hN7uGV04G1QAUzizeznmbWz8z6+VZ5EahrZtuA5cATzrkfUyeyiIi3cmfLzaB6g9g/cD9T2k7BMLp91I1Sb5fi1dWvBtwLt8meuklLmroRkfTEOcfS/Ut5fc3rLNu/jJyhOeldrTcDaw2kRJ4SaZIhLaZuREQyLDOjWZlmf71w2/amtryz4R3KDC9Dx7kdifkuxtN8KnoRET+KKBzBe+3eY/9D+3m49sMs/HohkeMjaTC5AR/t+ohz58+leSYVvYhIKiiWuxhDmw0l/tF43mz2JodOHKLtzLbcNPImRmwYwS9//pJmWVT0IiKpKFfWXDxS5xH2PrSXWXfPokCOAgz4bADFhhXjyWVPpsnlDlX0IiJpIHOmzLSv3J61Pdeypscabi19K6+veZ1Sb5ei07xOqTqPr6NuREQ8cvDEQYavH86ETRP4+c+fiSoRxeLOi8mWOdtV35aOuhERCUAl85TkzeZv/jWPXy5fuWsq+SvJ7PdbFBGRq3JhHj+1aI9eRCTIqehFRIKcil5EJMip6EVEgpyKXkQkyKnoRUSCnIpeRCTIqehFRIJcQJ4CwcwSgEPX+O0FgEC8upVyXb1AzaZcV0e5rt61ZCvhnCt4qYGALPqUMLPopM734CXlunqBmk25ro5yXT1/Z9PUjYhIkFPRi4gEuWAs+nFeB0iCcl29QM2mXFdHua6eX7MF3Ry9iIj8XTDu0YuIyEWCpujNrIWZ7TazvWb2pIc5ipnZCjOLM7MdZjbQt/x5M/vWzGJ9Hy09ynfQzLb5MkT7luUzs6Vmtsf3b940zlThou0Sa2anzOxhL7aZmU0ys6Nmtv2iZZfcPpZouO8xt9XMqnmQ7XUz2+W7//lmlse3vKSZ/X7RthuTxrmS/N2Z2X9822y3mTVP41wzL8p00MxifcvTcnsl1RGp9zhzzqX7DyAE2AeUBkKBLUAlj7L8C6jm+/x64GugEvA88HgAbKuDQIF/LHsNeNL3+ZPAqx7/Lr8HSnixzYAooBqw/UrbB2gJfAYYUBtY70G2ZkBm3+evXpSt5MXreZDrkr873/+FLUBWoJTv/21IWuX6x/gbwLMebK+kOiLVHmfBskdfE9jrnNvvnPsTmAG08SKIc+6Ic26T7/OfgTigiBdZrkIbYIrv8ylAWw+zNAH2Oeeu9Q1zKeKcWwX89I/FSW2fNsBUl2gdkMfM/pWW2ZxzS5xzZ31frgOKptb9X02uy2gDzHDO/eGcOwDsJfH/b5rmMjMD7gGmp8Z9X85lOiLVHmfBUvRFgG8u+jqeAChXMysJ3Ays9y160PfUa1JaT49cxAFLzCzGzPr4lhVyzh2BxAchcINH2QDu5e//+QJhmyW1fQLtcdeDxD2/C0qZ2WYz+8LM6nuQ51K/u0DZZvWBH5xzey5alubb6x8dkWqPs2ApervEMk8PJzKznMBc4GHn3ClgNFAGiACOkPi00Qv1nHPVgNuA/mYW5VGO/8fMQoHWwGzfokDZZkkJmMedmT0FnAWm+RYdAYo7524GHgU+MLNcaRgpqd9doGyzDvx9hyLNt9clOiLJVS+x7Kq2WbAUfTxQ7KKviwLfeZQFM8tC4i9wmnNuHoBz7gfn3Dnn3HlgPKn0dPVKnHPf+f49Csz35fjhwlNB379HvchG4h+fTc65H3wZA2KbkfT2CYjHnZndB9wBdHK+SV3f1Mgx3+cxJM6Fl0+rTJf53Xm+zcwsM3AnMPPCsrTeXpfqCFLxcRYsRb8RKGdmpXx7hfcCC7wI4pv7mwjEOefevGj5xXNq7YDt//zeNMh2nZldf+FzEl/I207itrrPt9p9wEdpnc3nb3tZgbDNfJLaPguArr6jImoDJy889U4rZtYCeAJo7Zz77aLlBc0sxPd5aaAcsD8NcyX1u1sA3GtmWc2slC/XhrTK5dMU2OWci7+wIC23V1IdQWo+ztLiVea0+CDxlemvSfxL/JSHOW4h8WnVViDW99ESeA/Y5lu+APiXB9lKk3jEwxZgx4XtBOQHlgN7fP/m8yBbDuAYkPuiZWm+zUj8Q3MEOEPinlTPpLYPiU+pR/oec9uASA+y7SVx/vbCY22Mb927fL/jLcAmoFUa50rydwc85dtmu4Hb0jKXb/lkoN8/1k3L7ZVUR6Ta40zvjBURCXLBMnUjIiJJUNGLiAQ5Fb2ISJBT0YuIBDkVvYhIkFPRi4gEORW9iEiQU9GLiAS5/wObZv5VV0RDiwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, label='losses', color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied the model on CPU just considering your convenience that you may run it on your laptop.\n",
    "The accuracy of prediction is low in part because the training epoches are not enough, I set it as small just to make a quick presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another form(CUDA/CPU), in this form, we can use the built-in optimizer which is much more efficient than we used above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still use the A and features defined above, now we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, A, feat_dim, hidden_dim, num_class, droprate=0.1):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        # we set two learnable parameters (torch.nn.Parameter)\n",
    "        self.W_N = torch.nn.Parameter(data=torch.rand(feat_dim, hidden_dim), requires_grad=True) \n",
    "        self.W_W = torch.nn.Parameter(data=torch.rand(hidden_dim, num_class), requires_grad=True)\n",
    "\n",
    "        #D = torch.Tensor(np.diag(np.array(np.power(np.sum(A.data.cpu().numpy(), axis=0),-1))))\n",
    "        self.D = torch.diag(pow(torch.sum(A, axis=0),-1))\n",
    "        self.A = A\n",
    "        self.droprate = droprate\n",
    "\n",
    "    def gcn_layer(self, A, D, X, W):\n",
    "        return torch.mm(torch.mm(torch.mm(D, A),X),W) #this is the gcn layer we defined, (D^{-1}A)XW\n",
    "    \n",
    "    def forward(self, X): #input -> hidden -> dropout -> logsoftmax, forward propagation\n",
    "        hidden = torch.nn.ReLU()(self.gcn_layer(self.A, self.D, X, self.W_N))\n",
    "        hidden = torch.nn.Dropout(p=self.droprate, inplace=False)(hidden)\n",
    "        output = self.gcn_layer(self.A, self.D, hidden, self.W_W)\n",
    "        output = torch.nn.LogSoftmax(dim=1)(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 001 loss_train: 15.205 loss_val: 13.363\n",
      "epoch: 002 loss_train: 12.610 loss_val: 11.538\n",
      "epoch: 003 loss_train: 11.026 loss_val: 9.663\n",
      "epoch: 004 loss_train: 10.098 loss_val: 8.970\n",
      "epoch: 005 loss_train: 9.917 loss_val: 9.555\n",
      "epoch: 006 loss_train: 8.958 loss_val: 8.014\n",
      "epoch: 007 loss_train: 8.947 loss_val: 8.168\n",
      "epoch: 008 loss_train: 7.404 loss_val: 7.481\n",
      "epoch: 009 loss_train: 8.239 loss_val: 7.717\n",
      "epoch: 010 loss_train: 7.243 loss_val: 7.167\n",
      "epoch: 011 loss_train: 6.771 loss_val: 6.991\n",
      "epoch: 012 loss_train: 6.579 loss_val: 5.796\n",
      "epoch: 013 loss_train: 6.140 loss_val: 5.821\n",
      "epoch: 014 loss_train: 5.595 loss_val: 5.856\n",
      "epoch: 015 loss_train: 5.510 loss_val: 6.050\n",
      "epoch: 016 loss_train: 5.170 loss_val: 5.030\n",
      "epoch: 017 loss_train: 4.651 loss_val: 4.658\n",
      "epoch: 018 loss_train: 4.442 loss_val: 5.001\n",
      "epoch: 019 loss_train: 4.253 loss_val: 4.686\n",
      "epoch: 020 loss_train: 3.960 loss_val: 5.079\n",
      "epoch: 021 loss_train: 4.604 loss_val: 4.907\n",
      "epoch: 022 loss_train: 4.141 loss_val: 4.801\n",
      "epoch: 023 loss_train: 3.754 loss_val: 4.718\n",
      "epoch: 024 loss_train: 3.812 loss_val: 4.186\n",
      "epoch: 025 loss_train: 3.561 loss_val: 3.980\n",
      "epoch: 026 loss_train: 3.743 loss_val: 4.125\n",
      "epoch: 027 loss_train: 3.214 loss_val: 3.373\n",
      "epoch: 028 loss_train: 3.257 loss_val: 3.172\n",
      "epoch: 029 loss_train: 3.080 loss_val: 3.344\n",
      "epoch: 030 loss_train: 2.793 loss_val: 3.125\n",
      "epoch: 031 loss_train: 2.813 loss_val: 3.141\n",
      "epoch: 032 loss_train: 3.029 loss_val: 3.322\n",
      "epoch: 033 loss_train: 2.756 loss_val: 2.800\n",
      "epoch: 034 loss_train: 2.497 loss_val: 3.019\n",
      "epoch: 035 loss_train: 2.400 loss_val: 2.855\n",
      "epoch: 036 loss_train: 2.490 loss_val: 2.723\n",
      "epoch: 037 loss_train: 2.270 loss_val: 2.452\n",
      "epoch: 038 loss_train: 2.100 loss_val: 2.519\n",
      "epoch: 039 loss_train: 2.149 loss_val: 2.453\n",
      "epoch: 040 loss_train: 2.070 loss_val: 2.707\n",
      "epoch: 041 loss_train: 2.110 loss_val: 2.463\n",
      "epoch: 042 loss_train: 2.117 loss_val: 2.205\n",
      "epoch: 043 loss_train: 2.131 loss_val: 2.357\n",
      "epoch: 044 loss_train: 2.031 loss_val: 2.153\n",
      "epoch: 045 loss_train: 1.912 loss_val: 2.077\n",
      "epoch: 046 loss_train: 1.891 loss_val: 2.132\n",
      "epoch: 047 loss_train: 1.839 loss_val: 2.087\n",
      "epoch: 048 loss_train: 1.694 loss_val: 1.856\n",
      "epoch: 049 loss_train: 1.668 loss_val: 1.800\n",
      "epoch: 050 loss_train: 1.573 loss_val: 1.921\n",
      "epoch: 051 loss_train: 1.689 loss_val: 1.931\n",
      "epoch: 052 loss_train: 1.528 loss_val: 1.903\n",
      "epoch: 053 loss_train: 1.485 loss_val: 1.662\n",
      "epoch: 054 loss_train: 1.562 loss_val: 2.009\n",
      "epoch: 055 loss_train: 1.567 loss_val: 1.757\n",
      "epoch: 056 loss_train: 1.549 loss_val: 1.581\n",
      "epoch: 057 loss_train: 1.384 loss_val: 1.682\n",
      "epoch: 058 loss_train: 1.513 loss_val: 1.795\n",
      "epoch: 059 loss_train: 1.501 loss_val: 1.548\n",
      "epoch: 060 loss_train: 1.344 loss_val: 1.602\n",
      "epoch: 061 loss_train: 1.344 loss_val: 1.479\n",
      "epoch: 062 loss_train: 1.238 loss_val: 1.422\n",
      "epoch: 063 loss_train: 1.209 loss_val: 1.361\n",
      "epoch: 064 loss_train: 1.241 loss_val: 1.461\n",
      "epoch: 065 loss_train: 1.304 loss_val: 1.378\n",
      "epoch: 066 loss_train: 1.190 loss_val: 1.500\n",
      "epoch: 067 loss_train: 1.152 loss_val: 1.255\n",
      "epoch: 068 loss_train: 1.231 loss_val: 1.248\n",
      "epoch: 069 loss_train: 1.101 loss_val: 1.359\n",
      "epoch: 070 loss_train: 1.079 loss_val: 1.278\n",
      "epoch: 071 loss_train: 1.093 loss_val: 1.263\n",
      "epoch: 072 loss_train: 1.047 loss_val: 1.254\n",
      "epoch: 073 loss_train: 1.100 loss_val: 1.216\n",
      "epoch: 074 loss_train: 1.072 loss_val: 1.131\n",
      "epoch: 075 loss_train: 1.105 loss_val: 1.355\n",
      "epoch: 076 loss_train: 1.012 loss_val: 1.362\n",
      "epoch: 077 loss_train: 0.999 loss_val: 1.104\n",
      "epoch: 078 loss_train: 0.941 loss_val: 1.146\n",
      "epoch: 079 loss_train: 1.017 loss_val: 1.223\n",
      "epoch: 080 loss_train: 0.969 loss_val: 1.074\n",
      "epoch: 081 loss_train: 0.898 loss_val: 1.086\n",
      "epoch: 082 loss_train: 0.899 loss_val: 1.068\n",
      "epoch: 083 loss_train: 0.938 loss_val: 1.081\n",
      "epoch: 084 loss_train: 0.960 loss_val: 1.116\n",
      "epoch: 085 loss_train: 0.911 loss_val: 0.924\n",
      "epoch: 086 loss_train: 0.840 loss_val: 1.055\n",
      "epoch: 087 loss_train: 0.905 loss_val: 0.944\n",
      "epoch: 088 loss_train: 0.868 loss_val: 0.925\n",
      "epoch: 089 loss_train: 0.810 loss_val: 0.991\n",
      "epoch: 090 loss_train: 0.788 loss_val: 0.973\n",
      "epoch: 091 loss_train: 0.823 loss_val: 0.918\n",
      "epoch: 092 loss_train: 0.753 loss_val: 0.942\n",
      "epoch: 093 loss_train: 0.825 loss_val: 1.011\n",
      "epoch: 094 loss_train: 0.784 loss_val: 0.955\n",
      "epoch: 095 loss_train: 0.671 loss_val: 0.868\n",
      "epoch: 096 loss_train: 0.715 loss_val: 0.863\n",
      "epoch: 097 loss_train: 0.750 loss_val: 0.875\n",
      "epoch: 098 loss_train: 0.729 loss_val: 0.844\n",
      "epoch: 099 loss_train: 0.702 loss_val: 0.846\n",
      "epoch: 100 loss_train: 0.729 loss_val: 0.880\n",
      "epoch: 101 loss_train: 0.667 loss_val: 0.797\n",
      "epoch: 102 loss_train: 0.682 loss_val: 0.835\n",
      "epoch: 103 loss_train: 0.682 loss_val: 0.831\n",
      "epoch: 104 loss_train: 0.667 loss_val: 0.837\n",
      "epoch: 105 loss_train: 0.707 loss_val: 0.899\n",
      "epoch: 106 loss_train: 0.637 loss_val: 0.717\n",
      "epoch: 107 loss_train: 0.625 loss_val: 0.733\n",
      "epoch: 108 loss_train: 0.656 loss_val: 0.696\n",
      "epoch: 109 loss_train: 0.577 loss_val: 0.783\n",
      "epoch: 110 loss_train: 0.609 loss_val: 0.730\n",
      "epoch: 111 loss_train: 0.582 loss_val: 0.809\n",
      "epoch: 112 loss_train: 0.623 loss_val: 0.786\n",
      "epoch: 113 loss_train: 0.593 loss_val: 0.784\n",
      "epoch: 114 loss_train: 0.579 loss_val: 0.741\n",
      "epoch: 115 loss_train: 0.573 loss_val: 0.722\n",
      "epoch: 116 loss_train: 0.567 loss_val: 0.671\n",
      "epoch: 117 loss_train: 0.549 loss_val: 0.806\n",
      "epoch: 118 loss_train: 0.566 loss_val: 0.777\n",
      "epoch: 119 loss_train: 0.574 loss_val: 0.715\n",
      "epoch: 120 loss_train: 0.553 loss_val: 0.750\n",
      "epoch: 121 loss_train: 0.526 loss_val: 0.777\n",
      "epoch: 122 loss_train: 0.525 loss_val: 0.642\n",
      "epoch: 123 loss_train: 0.545 loss_val: 0.662\n",
      "epoch: 124 loss_train: 0.550 loss_val: 0.692\n",
      "epoch: 125 loss_train: 0.535 loss_val: 0.699\n",
      "epoch: 126 loss_train: 0.519 loss_val: 0.656\n",
      "epoch: 127 loss_train: 0.512 loss_val: 0.664\n",
      "epoch: 128 loss_train: 0.490 loss_val: 0.752\n",
      "epoch: 129 loss_train: 0.516 loss_val: 0.674\n",
      "epoch: 130 loss_train: 0.490 loss_val: 0.607\n",
      "epoch: 131 loss_train: 0.471 loss_val: 0.656\n",
      "epoch: 132 loss_train: 0.485 loss_val: 0.575\n",
      "epoch: 133 loss_train: 0.514 loss_val: 0.666\n",
      "epoch: 134 loss_train: 0.476 loss_val: 0.561\n",
      "epoch: 135 loss_train: 0.462 loss_val: 0.648\n",
      "epoch: 136 loss_train: 0.463 loss_val: 0.603\n",
      "epoch: 137 loss_train: 0.474 loss_val: 0.542\n",
      "epoch: 138 loss_train: 0.462 loss_val: 0.560\n",
      "epoch: 139 loss_train: 0.466 loss_val: 0.591\n",
      "epoch: 140 loss_train: 0.446 loss_val: 0.617\n",
      "epoch: 141 loss_train: 0.451 loss_val: 0.590\n",
      "epoch: 142 loss_train: 0.445 loss_val: 0.630\n",
      "epoch: 143 loss_train: 0.439 loss_val: 0.549\n",
      "epoch: 144 loss_train: 0.453 loss_val: 0.651\n",
      "epoch: 145 loss_train: 0.444 loss_val: 0.563\n",
      "epoch: 146 loss_train: 0.448 loss_val: 0.569\n",
      "epoch: 147 loss_train: 0.440 loss_val: 0.533\n",
      "epoch: 148 loss_train: 0.434 loss_val: 0.550\n",
      "epoch: 149 loss_train: 0.459 loss_val: 0.582\n",
      "epoch: 150 loss_train: 0.434 loss_val: 0.558\n",
      "epoch: 151 loss_train: 0.416 loss_val: 0.624\n",
      "epoch: 152 loss_train: 0.430 loss_val: 0.572\n",
      "epoch: 153 loss_train: 0.415 loss_val: 0.587\n",
      "epoch: 154 loss_train: 0.388 loss_val: 0.517\n",
      "epoch: 155 loss_train: 0.398 loss_val: 0.533\n",
      "epoch: 156 loss_train: 0.411 loss_val: 0.617\n",
      "epoch: 157 loss_train: 0.385 loss_val: 0.535\n",
      "epoch: 158 loss_train: 0.379 loss_val: 0.542\n",
      "epoch: 159 loss_train: 0.394 loss_val: 0.560\n",
      "epoch: 160 loss_train: 0.417 loss_val: 0.527\n",
      "epoch: 161 loss_train: 0.371 loss_val: 0.510\n",
      "epoch: 162 loss_train: 0.381 loss_val: 0.533\n",
      "epoch: 163 loss_train: 0.388 loss_val: 0.539\n",
      "epoch: 164 loss_train: 0.373 loss_val: 0.522\n",
      "epoch: 165 loss_train: 0.403 loss_val: 0.513\n",
      "epoch: 166 loss_train: 0.374 loss_val: 0.497\n",
      "epoch: 167 loss_train: 0.371 loss_val: 0.528\n",
      "epoch: 168 loss_train: 0.375 loss_val: 0.511\n",
      "epoch: 169 loss_train: 0.368 loss_val: 0.532\n",
      "epoch: 170 loss_train: 0.371 loss_val: 0.534\n",
      "epoch: 171 loss_train: 0.342 loss_val: 0.449\n",
      "epoch: 172 loss_train: 0.365 loss_val: 0.530\n",
      "epoch: 173 loss_train: 0.348 loss_val: 0.516\n",
      "epoch: 174 loss_train: 0.370 loss_val: 0.548\n",
      "epoch: 175 loss_train: 0.338 loss_val: 0.492\n",
      "epoch: 176 loss_train: 0.375 loss_val: 0.520\n",
      "epoch: 177 loss_train: 0.344 loss_val: 0.516\n",
      "epoch: 178 loss_train: 0.359 loss_val: 0.500\n",
      "epoch: 179 loss_train: 0.337 loss_val: 0.522\n",
      "epoch: 180 loss_train: 0.357 loss_val: 0.543\n",
      "epoch: 181 loss_train: 0.357 loss_val: 0.484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 182 loss_train: 0.361 loss_val: 0.513\n",
      "epoch: 183 loss_train: 0.328 loss_val: 0.472\n",
      "epoch: 184 loss_train: 0.334 loss_val: 0.550\n",
      "epoch: 185 loss_train: 0.346 loss_val: 0.518\n",
      "epoch: 186 loss_train: 0.345 loss_val: 0.468\n",
      "epoch: 187 loss_train: 0.334 loss_val: 0.612\n",
      "epoch: 188 loss_train: 0.337 loss_val: 0.497\n",
      "epoch: 189 loss_train: 0.336 loss_val: 0.458\n",
      "epoch: 190 loss_train: 0.325 loss_val: 0.497\n",
      "epoch: 191 loss_train: 0.351 loss_val: 0.581\n",
      "epoch: 192 loss_train: 0.343 loss_val: 0.531\n",
      "epoch: 193 loss_train: 0.339 loss_val: 0.540\n",
      "epoch: 194 loss_train: 0.334 loss_val: 0.555\n",
      "epoch: 195 loss_train: 0.323 loss_val: 0.495\n",
      "epoch: 196 loss_train: 0.305 loss_val: 0.471\n",
      "epoch: 197 loss_train: 0.334 loss_val: 0.444\n",
      "epoch: 198 loss_train: 0.317 loss_val: 0.472\n",
      "epoch: 199 loss_train: 0.312 loss_val: 0.474\n",
      "epoch: 200 loss_train: 0.324 loss_val: 0.464\n"
     ]
    }
   ],
   "source": [
    "#automatic detect your gpu, if you do not have gpu, no worry, it will train on cpu\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "model = GCN(A.to(device), feat_dim, 16, num_class, droprate=0.5)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features.to(device))\n",
    "\n",
    "    loss = torch.nn.NLLLoss()(output[train_mask].to(device), label_list[train_mask].long().to(device))\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    loss_val = torch.nn.NLLLoss()(output[test_mask].to(device), label_list[test_mask].long().to(device))\n",
    "\n",
    "    print('epoch: {:03d}'.format(epoch+1),\n",
    "          'loss_train: {:.3f}'.format(loss.item()),\n",
    "          'loss_val: {:.3f}'.format(loss_val.item()))\n",
    "\n",
    "    losses.append(loss.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(losses, label='losses', color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of in test set:  0.856\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_, prediction = model(features.cuda()).max(dim=1)\n",
    "acc = acc_calc(label_list[test_mask].to(device),prediction[test_mask].to(device))\n",
    "print(\"accuracy of in test set: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe still have a gap between our model and the model assigned in paper--code can be see in(https://github.com/tkipf/gcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
