{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Numpy and Torch Implementation for GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here GCN is implemented according to the paper \"Semi-Supervised Classification with Graph Convolutional Networks\"  Thomas N. Kipf,Max Welling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GCNPNG.PNG\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1). Adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [1, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0]],\n",
    "    dtype=float\n",
    ").T\n",
    "\n",
    "display(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are the same as the adjacency matrix above, this will be helpful for the our following adjacency matrix construction\n"
     ]
    }
   ],
   "source": [
    "def adjacencyBuild(n, neg, num):\n",
    "    \"\"\"n: current node, neg: neighbour node, num: number of the nodes\"\"\"\n",
    "    \n",
    "    if len(n) != len(neg):\n",
    "        print(\"error\")\n",
    "        return \n",
    "    N = len(n)\n",
    "    A = np.zeros((num, num))\n",
    "    for i in range(N):\n",
    "        A[n[i]-1, neg[i]-1] = 1\n",
    "    return A.T\n",
    "        \n",
    "node = [1,2,2,3,4,4,5]\n",
    "neig = [3,1,3,5,3,5,1]\n",
    "\n",
    "testA = adjacencyBuild(node, neig, 5)\n",
    "display(testA)\n",
    "print(\"There are the same as the adjacency matrix above, this will be helpful for the our following adjacency matrix construction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2). Define 2-channel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  1],\n",
       "       [ 2, -1],\n",
       "       [ 1, -1],\n",
       "       [ 3, -3],\n",
       "       [ 5,  0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = np.array([\n",
    "    [6, 1],\n",
    "    [2, -1],\n",
    "    [1, -1],\n",
    "    [3, -3],\n",
    "    [5, 0],\n",
    "])\n",
    "\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X(features):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6,  1],\n",
       "       [ 2, -1],\n",
       "       [ 1, -1],\n",
       "       [ 3, -3],\n",
       "       [ 5,  0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AX:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7., -1.],\n",
       "       [ 0.,  0.],\n",
       "       [11., -3.],\n",
       "       [ 0.,  0.],\n",
       "       [ 4., -4.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"A:\")\n",
    "display(A)\n",
    "print(\"X(features):\")\n",
    "display(features)\n",
    "print(\"AX:\")\n",
    "display(A.dot(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph convolutional layer represents each node as the aggregation of its adjacent nodes.\n",
    "As we can see, the representation of each node (each row) is the sum of the features of its adjacent nodes (the adjacent nodes that the node points to).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3). Add self loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "I = np.eye(A.shape[0])\n",
    "display(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_hat = A + I\n",
    "display(A_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A*X:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13.,  0.],\n",
       "       [ 2., -1.],\n",
       "       [12., -4.],\n",
       "       [ 3., -3.],\n",
       "       [ 9., -4.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"A*X:\")\n",
    "display(A_hat.dot(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0., 0.],\n",
       "       [0., 3., 0., 0., 0.],\n",
       "       [0., 0., 2., 0., 0.],\n",
       "       [0., 0., 0., 3., 0.],\n",
       "       [0., 0., 0., 0., 2.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D = np.diag(np.array(np.sum(A_hat, axis=0)))\n",
    "display(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.33333333, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.5       , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.33333333, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.5       ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.70710678, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.57735027, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.70710678]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D_half = np.diag(np.array(np.power(np.sum(A_hat, axis=0),-0.5)))\n",
    "display(D_half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4). normalization(two form, note spectral GCN should use the first form)\n",
    "\n",
    "$\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}X$\n",
    "\n",
    "From stefan:\n",
    "\n",
    "there might be numerical instabilities because the filters are not bounded functions outside the interval [-1,1] (or [-2,2])\n",
    "in fact, chebyshev polynomials grow extremely fast outside that interval and if A has eigenvalues there this might cause numerical instabilities, it would probably \"work\" in many case , but certainly not be the \"right thing to do\" scientifically :wink:\n",
    "\n",
    "\n",
    "$\\hat{D}^{-1}\\hat{A}X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization1:\n",
      "\n",
      "[[17.86575026  0.25951302]\n",
      " [ 3.46410162 -1.73205081]\n",
      " [15.67299763 -4.61880215]\n",
      " [ 5.19615242 -5.19615242]\n",
      " [11.94938299 -4.87831518]]\n",
      "normalization2:\n",
      "\n",
      "[[ 6.5         0.        ]\n",
      " [ 0.66666667 -0.33333333]\n",
      " [ 6.         -2.        ]\n",
      " [ 1.         -1.        ]\n",
      " [ 4.5        -2.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"normalization1:\\n\")#np.linalg.matrix_power(D, -1)\n",
    "print(np.dot(np.dot(np.dot(D,A_hat),D_half), features))\n",
    "print(\"normalization2:\\n\")#np.linalg.matrix_power(D, -1)\n",
    "print(np.dot(np.dot(np.linalg.inv(D),A_hat), features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5). Add weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([\n",
    "             [1, -0.5],\n",
    "             [-0.6, 1]\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add weights:\n",
      "\n",
      "[[ 3.8  -2.25]\n",
      " [ 0.    0.  ]\n",
      " [ 6.4  -4.25]\n",
      " [ 0.    0.  ]\n",
      " [ 3.2  -3.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"add weights:\\n\")\n",
    "print(np.dot(np.dot(np.dot(np.linalg.inv(D),A), features),W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add activation function(tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9989996 , -0.97802611],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.99999448, -0.99959315],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.9966824 , -0.99505475]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(np.tanh(np.dot(np.dot(np.dot(np.linalg.inv(D),A), features),W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97811873, 0.09534946],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.9983412 , 0.01406363],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.96083428, 0.04742587]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigmoid = lambda x:1/(1+np.exp(-x))\n",
    "display(sigmoid(np.dot(np.dot(np.dot(np.linalg.inv(D),A), features),W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Application in Cora Dataset\n",
    "\n",
    "Here, a semi-supervised task about Cora Dataset is implemented step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#do a little modified compared to before, it is symmetric matrix form\n",
    "def adjacencyBuild(n, neg, num):\n",
    "    \"\"\"n: current node, neg: neighbour node, num: number of the nodes\"\"\"\n",
    "    if len(n) != len(neg):\n",
    "        print(\"error\")\n",
    "        return \n",
    "    N = len(n)\n",
    "    A = np.zeros((num, num))\n",
    "    for i in range(N):\n",
    "        A[n[i], neg[i]] = 1\n",
    "        A[neg[i], n[i]] = 1\n",
    "    return A\n",
    "\n",
    "\n",
    "#define a accuracy calculation, (correct prediction)/ total\n",
    "def acc_calc(true, pred):\n",
    "    correct = pred.eq(true).sum().item()\n",
    "    total = test_mask.sum().item()\n",
    "    return (correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1). data prepocessing(you can ignore this part, but do figure out what the data format is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.Tensor(np.load(\"data/train_mask.npy\")).bool() # [True, False,...,True], size: 140\n",
    "test_mask = torch.Tensor(np.load(\"data/test_mask.npy\")).bool() # [True, False,...,True], size: 1000\n",
    "\n",
    "features = torch.Tensor(np.load(\"data/features.npy\"))\n",
    "label_list = torch.Tensor(np.load(\"data/label_list.npy\"))\n",
    "edge_index = np.load(\"data/edge_index.npy\")\n",
    "feat_dim = features.shape[1]\n",
    "num_class = int(label_list.max().data.numpy() + 1) #7\n",
    "\n",
    "node_num = features.shape[0]\n",
    "A = adjacencyBuild(edge_index[0,:],edge_index[1,:], node_num)\n",
    "A = A + np.eye(A.shape[0])\n",
    "A = torch.Tensor(A)\n",
    "D = torch.diag(pow(torch.sum(A, axis=0),-1/2)) #nomilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** data info******************************\n",
      "number of node : 2708\n",
      "number of edges : 5278\n",
      "number of classes : 7\n",
      "channel of node features : 1433\n",
      "shape of feat_Matrix : torch.Size([2708, 1433])\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"{} data info{}\".format(\"*\"*30, \"*\"*30))\n",
    "print(\"number of node : {}\".format(node_num))\n",
    "print(\"number of edges : {}\".format(round(edge_index.shape[1]/2))) # maybe mistake\n",
    "print(\"number of classes : {}\".format(num_class))\n",
    "print(\"channel of node features : {}\".format(feat_dim))\n",
    "print(\"shape of feat_Matrix : {}\".format(features.shape))\n",
    "print(\"{}***********{}\".format(\"*\"*30, \"*\"*30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2). This part show you some basic principles, you can also jump this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = lambda x:np.maximum(0,x)\n",
    "sigmoid = lambda x:1/(1+np.exp(-x))\n",
    "\n",
    "def gcn_layer(A, D, X, W, activation = 'relu'):\n",
    "    if activation == 'relu':\n",
    "        return relu(np.dot(np.dot(np.dot(np.linalg.inv(D),A),X),W))\n",
    "    elif activation == 'sigmoid':\n",
    "        return sigmoid(np.dot(np.dot(np.dot(np.linalg.inv(D),A),X),W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_N = np.random.normal(loc=0, scale=1, size=(feat_dim, node_num))\n",
    "W_W = np.random.normal(loc=0, size=(node_num, num_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = gcn_layer(A, D, features, W_N, activation = 'relu')\n",
    "output = gcn_layer(A, D, hidden, W_W, activation = 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.nn.Softmax(dim=1)(torch.Tensor(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 7])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 3,  ..., 4, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result.size())\n",
    "display(result)\n",
    "display(torch.argmax(result,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9488)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(result, label_list.long())\n",
    "display(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3).Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_layer(A, D, X, W, activation = 'relu'):\n",
    "    if activation == 'relu':\n",
    "        return torch.relu(torch.mm(torch.mm(torch.mm(torch.mm(D, A),D),X),W))\n",
    "    elif activation == 'sigmoid':\n",
    "        return torch.sigmoid(torch.mm(torch.mm(torch.mm(torch.mm(D, A),D),X),W))\n",
    "    else:\n",
    "        return torch.mm(torch.mm(torch.mm(torch.mm(D, A),D),X),W)\n",
    "    \n",
    "def two_layer_gcn(A, D, X, W1, W2):\n",
    "    hidden = gcn_layer(A, D, X, W1, activation = None) # we do not add any activation\n",
    "    output = gcn_layer(A, D, hidden, W2, activation = None)\n",
    "    return torch.nn.Softmax(dim=1)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_N = np.random.normal(loc=0, scale=1, size=(feat_dim, 50))\n",
    "W_W = np.random.normal(loc=0, size=(50, num_class))\n",
    "\n",
    "# both of the following parameters will updated during training because requires_grad=True\n",
    "W_N = torch.autograd.Variable(torch.Tensor(W_N), requires_grad=True) \n",
    "W_W = torch.autograd.Variable(torch.Tensor(W_W), requires_grad=True) \n",
    "\n",
    "A = torch.Tensor(A)\n",
    "D = torch.Tensor(D)\n",
    "features = torch.Tensor(features)\n",
    "\n",
    "learning_rate = 0.5 # the smaller it would be better, but after that, you need to increase the size of epoches\n",
    "losses = []\n",
    "epoches = 250\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if we do not train, the accuracy is : 0.146000\n"
     ]
    }
   ],
   "source": [
    "prediction = two_layer_gcn(A, D, features, W_N, W_W)\n",
    "prediction = torch.argmax(prediction,dim=1)\n",
    "correct = float(prediction[test_mask].eq(label_list[test_mask]).sum().item())\n",
    "accuracy = correct / test_mask.sum().item()\n",
    "print('if we do not train, the accuracy is : {:.6f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:1.9907597303390503\n",
      "loss:1.9030730724334717\n",
      "loss:1.879948377609253\n",
      "loss:1.8589141368865967\n",
      "loss:1.8543177843093872\n",
      "loss:1.8528002500534058\n",
      "loss:1.8461103439331055\n",
      "loss:1.8453397750854492\n",
      "loss:1.8448760509490967\n",
      "loss:1.8444111347198486\n",
      "loss:1.8438116312026978\n",
      "loss:1.8434792757034302\n",
      "loss:1.8432178497314453\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoches):\n",
    "    # hidden = gcn_layer(A_hat, D, feat_Matrix, W_N, activation = 'relu')\n",
    "    # output = gcn_layer(A_hat, D, hidden, W_W, activation = 'relu')\n",
    "    # result = torch.nn.Softmax(dim=1)(output)\n",
    "    \n",
    "    result = two_layer_gcn(A, D, features, W_N, W_W)\n",
    "    loss = loss_fn(result[train_mask], label_list[train_mask].long())\n",
    "    \n",
    "    loss.backward()\n",
    "    W_N.data.add_(-learning_rate*W_N.grad.data)\n",
    "    W_W.data.add_(-learning_rate*W_W.grad.data) # this optimizer defined manually is very bad, do not converge efficiently\n",
    "    \n",
    "    W_N.grad.data.zero_() # pytorch will accumulate the gradient, we need to clean it after each epoch training\n",
    "    W_W.grad.data.zero_() \n",
    "    losses.append(loss.data.numpy())\n",
    "    if i % 20 == 0: print('loss:{}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training, the accuracy: 0.211000\n"
     ]
    }
   ],
   "source": [
    "prediction = two_layer_gcn(A, D, features, W_N, W_W)\n",
    "prediction = torch.argmax(prediction,dim=1)\n",
    "correct = float(prediction[test_mask].eq(label_list[test_mask]).sum().item())\n",
    "accuracy = correct / test_mask.sum().item()\n",
    "print('after training, the accuracy: {:.6f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, label='losses', color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied the model on CPU just considering your convenience that you may run it on your laptop.\n",
    "The accuracy of prediction is low in part because the training epoches are not enough, I set it as small just to make a quick presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another form(CUDA/CPU), in this form, we can use the pytorch built-in optimizer which is much more efficient than we used above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still use the A and features defined above, now we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class GCNlayer(torch.nn.Module):\n",
    "    def __init__(self, feat_dim, out_dim, bias=False):\n",
    "        super(GCNlayer, self).__init__()\n",
    "        # we set two learnable parameters self.W and self.bias (torch.nn.Parameter)\n",
    "        self.W = torch.nn.Parameter(data=torch.rand(feat_dim, out_dim), requires_grad=True) \n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(\n",
    "                data=torch.empty(1,out_dim).uniform_(0,0.05), requires_grad=True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "    def forward(self,A,X): \n",
    "        D = torch.diag(pow(torch.sum(A, axis=0),-1/2))\n",
    "        if self.bias != None:\n",
    "            output = torch.mm(torch.mm(torch.mm(torch.mm(D, A),D),X),self.W) + self.bias\n",
    "        else:\n",
    "            output = torch.mm(torch.mm(torch.mm(torch.mm(D, A),D),X),self.W)\n",
    "        return(output)\n",
    "    \n",
    "    \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, feat_dim, hidden_dim, num_class, droprate=0.1, bias=True):\n",
    "        super(GCN, self).__init__()\n",
    "        self.GConv1 = GCNlayer(feat_dim, hidden_dim, bias)\n",
    "        self.GConv2 = GCNlayer(hidden_dim, num_class, bias)\n",
    "        self.dropout = torch.nn.Dropout(p=droprate, inplace=False)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, A, X): #input -> hidden -> dropout -> logsoftmax, forward propagation\n",
    "        #D = torch.diag(pow(torch.sum(A, axis=0),-1/2))\n",
    "        hidden = self.relu(self.GConv1(A, X)) \n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.GConv2(A, hidden)\n",
    "        output = torch.nn.LogSoftmax(dim=1)(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 001 loss_train: 12.333 loss_val: 9.069\n",
      "epoch: 002 loss_train: 10.961 loss_val: 7.978\n",
      "epoch: 003 loss_train: 10.136 loss_val: 7.148\n",
      "epoch: 004 loss_train: 7.998 loss_val: 7.001\n",
      "epoch: 005 loss_train: 7.684 loss_val: 6.443\n",
      "epoch: 006 loss_train: 6.653 loss_val: 6.587\n",
      "epoch: 007 loss_train: 6.922 loss_val: 6.339\n",
      "epoch: 008 loss_train: 6.287 loss_val: 6.962\n",
      "epoch: 009 loss_train: 5.920 loss_val: 6.581\n",
      "epoch: 010 loss_train: 5.623 loss_val: 6.397\n",
      "epoch: 011 loss_train: 5.935 loss_val: 6.344\n",
      "epoch: 012 loss_train: 5.387 loss_val: 6.329\n",
      "epoch: 013 loss_train: 4.804 loss_val: 5.742\n",
      "epoch: 014 loss_train: 5.297 loss_val: 5.922\n",
      "epoch: 015 loss_train: 4.870 loss_val: 5.652\n",
      "epoch: 016 loss_train: 4.935 loss_val: 6.049\n",
      "epoch: 017 loss_train: 5.085 loss_val: 5.490\n",
      "epoch: 018 loss_train: 4.057 loss_val: 5.349\n",
      "epoch: 019 loss_train: 4.528 loss_val: 5.084\n",
      "epoch: 020 loss_train: 4.026 loss_val: 5.053\n",
      "epoch: 021 loss_train: 3.597 loss_val: 4.616\n",
      "epoch: 022 loss_train: 3.266 loss_val: 4.431\n",
      "epoch: 023 loss_train: 3.460 loss_val: 4.174\n",
      "epoch: 024 loss_train: 3.274 loss_val: 3.758\n",
      "epoch: 025 loss_train: 3.091 loss_val: 3.845\n",
      "epoch: 026 loss_train: 2.797 loss_val: 3.700\n",
      "epoch: 027 loss_train: 2.840 loss_val: 3.676\n",
      "epoch: 028 loss_train: 2.510 loss_val: 3.306\n",
      "epoch: 029 loss_train: 2.515 loss_val: 3.097\n",
      "epoch: 030 loss_train: 2.458 loss_val: 3.687\n",
      "epoch: 031 loss_train: 2.219 loss_val: 3.139\n",
      "epoch: 032 loss_train: 2.157 loss_val: 3.226\n",
      "epoch: 033 loss_train: 2.066 loss_val: 3.128\n",
      "epoch: 034 loss_train: 1.869 loss_val: 2.979\n",
      "epoch: 035 loss_train: 1.688 loss_val: 2.778\n",
      "epoch: 036 loss_train: 2.025 loss_val: 2.928\n",
      "epoch: 037 loss_train: 1.845 loss_val: 3.003\n",
      "epoch: 038 loss_train: 1.688 loss_val: 2.805\n",
      "epoch: 039 loss_train: 1.635 loss_val: 2.467\n",
      "epoch: 040 loss_train: 1.842 loss_val: 2.938\n",
      "epoch: 041 loss_train: 1.381 loss_val: 2.672\n",
      "epoch: 042 loss_train: 1.489 loss_val: 2.737\n",
      "epoch: 043 loss_train: 1.711 loss_val: 2.677\n",
      "epoch: 044 loss_train: 1.488 loss_val: 2.871\n",
      "epoch: 045 loss_train: 1.756 loss_val: 2.813\n",
      "epoch: 046 loss_train: 1.393 loss_val: 2.620\n",
      "epoch: 047 loss_train: 1.583 loss_val: 2.364\n",
      "epoch: 048 loss_train: 1.423 loss_val: 2.494\n",
      "epoch: 049 loss_train: 1.125 loss_val: 2.414\n",
      "epoch: 050 loss_train: 1.136 loss_val: 2.191\n",
      "epoch: 051 loss_train: 1.358 loss_val: 2.359\n",
      "epoch: 052 loss_train: 1.402 loss_val: 2.600\n",
      "epoch: 053 loss_train: 1.051 loss_val: 2.203\n",
      "epoch: 054 loss_train: 0.930 loss_val: 2.270\n",
      "epoch: 055 loss_train: 1.108 loss_val: 2.064\n",
      "epoch: 056 loss_train: 1.112 loss_val: 2.077\n",
      "epoch: 057 loss_train: 1.222 loss_val: 2.068\n",
      "epoch: 058 loss_train: 1.076 loss_val: 2.085\n",
      "epoch: 059 loss_train: 1.158 loss_val: 2.135\n",
      "epoch: 060 loss_train: 0.929 loss_val: 1.880\n",
      "epoch: 061 loss_train: 0.910 loss_val: 1.887\n",
      "epoch: 062 loss_train: 0.776 loss_val: 2.035\n",
      "epoch: 063 loss_train: 1.180 loss_val: 1.874\n",
      "epoch: 064 loss_train: 0.850 loss_val: 1.835\n",
      "epoch: 065 loss_train: 0.944 loss_val: 1.786\n",
      "epoch: 066 loss_train: 0.843 loss_val: 1.877\n",
      "epoch: 067 loss_train: 0.778 loss_val: 1.810\n",
      "epoch: 068 loss_train: 0.760 loss_val: 1.991\n",
      "epoch: 069 loss_train: 0.804 loss_val: 2.062\n",
      "epoch: 070 loss_train: 0.929 loss_val: 1.817\n",
      "epoch: 071 loss_train: 0.760 loss_val: 1.844\n",
      "epoch: 072 loss_train: 0.758 loss_val: 1.966\n",
      "epoch: 073 loss_train: 0.751 loss_val: 1.749\n",
      "epoch: 074 loss_train: 0.664 loss_val: 1.920\n",
      "epoch: 075 loss_train: 0.545 loss_val: 1.837\n",
      "epoch: 076 loss_train: 0.665 loss_val: 1.873\n",
      "epoch: 077 loss_train: 0.677 loss_val: 1.757\n",
      "epoch: 078 loss_train: 0.512 loss_val: 1.709\n",
      "epoch: 079 loss_train: 0.511 loss_val: 1.759\n",
      "epoch: 080 loss_train: 0.613 loss_val: 1.672\n",
      "epoch: 081 loss_train: 0.478 loss_val: 1.729\n",
      "epoch: 082 loss_train: 0.567 loss_val: 1.612\n",
      "epoch: 083 loss_train: 0.478 loss_val: 1.680\n",
      "epoch: 084 loss_train: 0.495 loss_val: 1.541\n",
      "epoch: 085 loss_train: 0.629 loss_val: 1.812\n",
      "epoch: 086 loss_train: 0.565 loss_val: 1.658\n",
      "epoch: 087 loss_train: 0.596 loss_val: 1.747\n",
      "epoch: 088 loss_train: 0.576 loss_val: 1.603\n",
      "epoch: 089 loss_train: 0.370 loss_val: 1.635\n",
      "epoch: 090 loss_train: 0.455 loss_val: 1.541\n",
      "epoch: 091 loss_train: 0.411 loss_val: 1.623\n",
      "epoch: 092 loss_train: 0.472 loss_val: 1.498\n",
      "epoch: 093 loss_train: 0.604 loss_val: 1.541\n",
      "epoch: 094 loss_train: 0.515 loss_val: 1.505\n",
      "epoch: 095 loss_train: 0.517 loss_val: 1.504\n",
      "epoch: 096 loss_train: 0.386 loss_val: 1.608\n",
      "epoch: 097 loss_train: 0.418 loss_val: 1.408\n",
      "epoch: 098 loss_train: 0.428 loss_val: 1.571\n",
      "epoch: 099 loss_train: 0.446 loss_val: 1.490\n",
      "epoch: 100 loss_train: 0.343 loss_val: 1.461\n",
      "epoch: 101 loss_train: 0.458 loss_val: 1.550\n",
      "epoch: 102 loss_train: 0.411 loss_val: 1.477\n",
      "epoch: 103 loss_train: 0.394 loss_val: 1.534\n",
      "epoch: 104 loss_train: 0.458 loss_val: 1.483\n",
      "epoch: 105 loss_train: 0.413 loss_val: 1.407\n",
      "epoch: 106 loss_train: 0.363 loss_val: 1.578\n",
      "epoch: 107 loss_train: 0.366 loss_val: 1.510\n",
      "epoch: 108 loss_train: 0.317 loss_val: 1.260\n",
      "epoch: 109 loss_train: 0.394 loss_val: 1.402\n",
      "epoch: 110 loss_train: 0.380 loss_val: 1.400\n",
      "epoch: 111 loss_train: 0.373 loss_val: 1.407\n",
      "epoch: 112 loss_train: 0.317 loss_val: 1.474\n",
      "epoch: 113 loss_train: 0.350 loss_val: 1.346\n",
      "epoch: 114 loss_train: 0.284 loss_val: 1.382\n",
      "epoch: 115 loss_train: 0.325 loss_val: 1.401\n",
      "epoch: 116 loss_train: 0.353 loss_val: 1.317\n",
      "epoch: 117 loss_train: 0.411 loss_val: 1.526\n",
      "epoch: 118 loss_train: 0.315 loss_val: 1.251\n",
      "epoch: 119 loss_train: 0.315 loss_val: 1.404\n",
      "epoch: 120 loss_train: 0.416 loss_val: 1.352\n",
      "epoch: 121 loss_train: 0.249 loss_val: 1.290\n",
      "epoch: 122 loss_train: 0.352 loss_val: 1.463\n",
      "epoch: 123 loss_train: 0.286 loss_val: 1.311\n",
      "epoch: 124 loss_train: 0.254 loss_val: 1.340\n",
      "epoch: 125 loss_train: 0.251 loss_val: 1.308\n",
      "epoch: 126 loss_train: 0.276 loss_val: 1.276\n",
      "epoch: 127 loss_train: 0.327 loss_val: 1.358\n",
      "epoch: 128 loss_train: 0.315 loss_val: 1.343\n",
      "epoch: 129 loss_train: 0.295 loss_val: 1.222\n",
      "epoch: 130 loss_train: 0.318 loss_val: 1.275\n",
      "epoch: 131 loss_train: 0.210 loss_val: 1.333\n",
      "epoch: 132 loss_train: 0.264 loss_val: 1.150\n",
      "epoch: 133 loss_train: 0.245 loss_val: 1.236\n",
      "epoch: 134 loss_train: 0.318 loss_val: 1.297\n",
      "epoch: 135 loss_train: 0.233 loss_val: 1.152\n",
      "epoch: 136 loss_train: 0.244 loss_val: 1.285\n",
      "epoch: 137 loss_train: 0.201 loss_val: 1.253\n",
      "epoch: 138 loss_train: 0.193 loss_val: 1.208\n",
      "epoch: 139 loss_train: 0.192 loss_val: 1.335\n",
      "epoch: 140 loss_train: 0.267 loss_val: 1.249\n",
      "epoch: 141 loss_train: 0.164 loss_val: 1.322\n",
      "epoch: 142 loss_train: 0.181 loss_val: 1.130\n",
      "epoch: 143 loss_train: 0.237 loss_val: 1.241\n",
      "epoch: 144 loss_train: 0.199 loss_val: 1.234\n",
      "epoch: 145 loss_train: 0.177 loss_val: 1.198\n",
      "epoch: 146 loss_train: 0.173 loss_val: 1.201\n",
      "epoch: 147 loss_train: 0.183 loss_val: 1.186\n",
      "epoch: 148 loss_train: 0.182 loss_val: 1.095\n",
      "epoch: 149 loss_train: 0.184 loss_val: 1.167\n",
      "epoch: 150 loss_train: 0.202 loss_val: 1.305\n",
      "epoch: 151 loss_train: 0.159 loss_val: 1.131\n",
      "epoch: 152 loss_train: 0.187 loss_val: 1.130\n",
      "epoch: 153 loss_train: 0.221 loss_val: 1.291\n",
      "epoch: 154 loss_train: 0.174 loss_val: 1.163\n",
      "epoch: 155 loss_train: 0.098 loss_val: 1.175\n",
      "epoch: 156 loss_train: 0.174 loss_val: 1.270\n",
      "epoch: 157 loss_train: 0.210 loss_val: 1.154\n",
      "epoch: 158 loss_train: 0.180 loss_val: 1.148\n",
      "epoch: 159 loss_train: 0.262 loss_val: 1.219\n",
      "epoch: 160 loss_train: 0.145 loss_val: 1.226\n",
      "epoch: 161 loss_train: 0.199 loss_val: 1.210\n",
      "epoch: 162 loss_train: 0.217 loss_val: 1.127\n",
      "epoch: 163 loss_train: 0.162 loss_val: 1.191\n",
      "epoch: 164 loss_train: 0.149 loss_val: 1.145\n",
      "epoch: 165 loss_train: 0.145 loss_val: 1.153\n",
      "epoch: 166 loss_train: 0.118 loss_val: 1.170\n",
      "epoch: 167 loss_train: 0.116 loss_val: 1.172\n",
      "epoch: 168 loss_train: 0.130 loss_val: 1.134\n",
      "epoch: 169 loss_train: 0.145 loss_val: 1.118\n",
      "epoch: 170 loss_train: 0.143 loss_val: 1.194\n",
      "epoch: 171 loss_train: 0.147 loss_val: 1.170\n",
      "epoch: 172 loss_train: 0.177 loss_val: 1.123\n",
      "epoch: 173 loss_train: 0.155 loss_val: 1.244\n",
      "epoch: 174 loss_train: 0.159 loss_val: 1.204\n",
      "epoch: 175 loss_train: 0.194 loss_val: 1.155\n",
      "epoch: 176 loss_train: 0.167 loss_val: 1.117\n",
      "epoch: 177 loss_train: 0.130 loss_val: 1.103\n",
      "epoch: 178 loss_train: 0.134 loss_val: 1.171\n",
      "epoch: 179 loss_train: 0.163 loss_val: 1.218\n",
      "epoch: 180 loss_train: 0.126 loss_val: 1.101\n",
      "epoch: 181 loss_train: 0.174 loss_val: 1.168\n",
      "epoch: 182 loss_train: 0.115 loss_val: 1.085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 183 loss_train: 0.172 loss_val: 1.143\n",
      "epoch: 184 loss_train: 0.096 loss_val: 1.093\n",
      "epoch: 185 loss_train: 0.122 loss_val: 1.145\n",
      "epoch: 186 loss_train: 0.099 loss_val: 1.167\n",
      "epoch: 187 loss_train: 0.151 loss_val: 1.071\n",
      "epoch: 188 loss_train: 0.093 loss_val: 1.201\n",
      "epoch: 189 loss_train: 0.167 loss_val: 1.112\n",
      "epoch: 190 loss_train: 0.127 loss_val: 1.086\n",
      "epoch: 191 loss_train: 0.073 loss_val: 1.083\n",
      "epoch: 192 loss_train: 0.110 loss_val: 1.105\n",
      "epoch: 193 loss_train: 0.067 loss_val: 1.062\n",
      "epoch: 194 loss_train: 0.111 loss_val: 1.005\n",
      "epoch: 195 loss_train: 0.104 loss_val: 1.129\n",
      "epoch: 196 loss_train: 0.113 loss_val: 1.054\n",
      "epoch: 197 loss_train: 0.111 loss_val: 1.070\n",
      "epoch: 198 loss_train: 0.102 loss_val: 1.055\n",
      "epoch: 199 loss_train: 0.073 loss_val: 1.077\n",
      "epoch: 200 loss_train: 0.146 loss_val: 1.097\n"
     ]
    }
   ],
   "source": [
    "# automatic detect your gpu, if you do not have gpu, no worry, it will train on cpu\n",
    "# normally, in industry, we need to split train, validation, test dataset; but usually, splitting into train and test set is enough\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "model = GCN(feat_dim, 16, num_class, droprate=0.5, bias=False)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()#pytorch will accumulate the gradient, we need to clean it after each epoch training\n",
    "    output = model(A.to(device), features.to(device))\n",
    "\n",
    "    loss = torch.nn.NLLLoss()(output[train_mask].to(device), label_list[train_mask].long().to(device)) # loss defined on train set\n",
    "    loss.backward(retain_graph=True) # back propagation\n",
    "    optimizer.step() # updated parameter\n",
    "    loss_val = torch.nn.NLLLoss()(output[test_mask].to(device), label_list[test_mask].long().to(device)) # loss defined on test set\n",
    "\n",
    "    print('epoch: {:03d}'.format(epoch+1),\n",
    "          'loss_train: {:.3f}'.format(loss.item()),\n",
    "          'loss_val: {:.3f}'.format(loss_val.item()))\n",
    "\n",
    "    losses.append(loss.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhU5d3/8fedZJKQjSVkYQthX4WIQbAoICgguIAoQlsFte7ytLXV6s/Lx5bS2qqPWqutRUWxoqJCiwurgAJl0RjDEvZFSCAkgbAkISHb/ftjJilbIGaZk0k+r+viyszJyZzv3DN85p7vnDnHWGsRERHf4+d0ASIiUj0KcBERH6UAFxHxUQpwEREfpQAXEfFRAd7cWMuWLW18fLw3Nyki4vO+/fbbw9baqLOXezXA4+PjSUpK8uYmRUR8njFm3/mWq4UiIuKjFOAiIj5KAS4i4qO82gMXETmf4uJi0tPTKSwsdLoURwUHB9O2bVtcLleV1leAi4jj0tPTCQ8PJz4+HmOM0+U4wlrLkSNHSE9Pp0OHDlX6G7VQRMRxhYWFREZGNtrwBjDGEBkZ+YPehSjARaReaMzhXe6HjoFPBPhnOz7jz6v/7HQZIiL1ik8E+JLdS/jTf/7kdBki0oCFhYU5XcIP5hMB3qJJC44VHqO0rNTpUkRE6o2LBrgxZqYxJssYs/m0Zc8ZY7YZYzYaY/5ljGlWl0W2aNICgGOFx+pyMyIiWGt59NFH6d27N5dccglz5swBICMjg8GDB5OQkEDv3r1ZtWoVpaWlTJkypWLdF198EYDdu3czatQoLrvsMq666iq2bdsGwEcffUTv3r3p27cvgwcPrnGtVdmN8G3gFeCd05YtBZ6w1pYYY/4MPAH8psbVVKJ5cHMAcgpyiAyJrKvNiEg98ItFvyDlUEqt3mZCbAIvjXqpSuvOmzePlJQUNmzYwOHDh+nfvz+DBw/mvffeY+TIkTz55JOUlpZy8uRJUlJSOHDgAJs3u+e3x465J5n33nsvr732Gl26dGH9+vU8+OCDLF++nGnTprF48WLatGlTsW5NXDTArbUrjTHxZy1bctrVdcAtNa7kAspn4EcLj9blZkREWL16NZMmTcLf35+YmBiGDBnCN998Q//+/bnrrrsoLi5m7NixJCQk0LFjR/bs2cPUqVMZM2YMI0aMIC8vjzVr1nDrrbdW3OapU6cAGDRoEFOmTGHChAncfPPNNa61Nr7IcxcwpxZup1LlAZ5TkFOXmxGReqCqM+W6UtmJ3gcPHszKlSv5/PPPuf3223n00Ue544472LBhA4sXL+bVV1/lww8/5KWXXqJZs2akpJz7LuK1115j/fr1fP755yQkJJCSkkJkZPW7CjX6ENMY8yRQAsy+wDr3GmOSjDFJ2dnZ1dpO8yb/baGIiNSlwYMHM2fOHEpLS8nOzmblypVcfvnl7Nu3j+joaO655x7uvvtukpOTOXz4MGVlZYwfP57f//73JCcnExERQYcOHfjoo48A9wvChg0bAHdvfMCAAUybNo2WLVuSlpZWo1qrPQM3xkwGrgeG28pesgBr7QxgBkBiYmKl611IRQulQC0UEalb48aNY+3atfTt2xdjDM8++yyxsbHMmjWL5557DpfLRVhYGO+88w4HDhzgzjvvpKysDIBnnnkGgNmzZ/PAAw8wffp0iouLmThxIn379uXRRx9l586dWGsZPnw4ffv2rVGt5gLZ+9+V3D3wz6y1vT3XRwEvAEOstVWeVicmJtrqnNChuLSYwOmBTBs6jaeGPPWD/15E6retW7fSo0cPp8uoF843FsaYb621iWevW5XdCN8H1gLdjDHpxpi7ce+VEg4sNcakGGNeq53Sz8/l7yI8MFwtFBGR01RlL5RJ51n8Zh3UckHNmzQnp1ABLiJSzie+iQnuPrh64CINV1XauQ3dDx0DnwpwtVBEGqbg4GCOHDnSqEO8/HjgwcHBVf4bnzmhQ/Pg5mzJ3uJ0GSJSB9q2bUt6ejrV3dW4oSg/I09V+UyAawYu0nC5XK4qn4VG/sunWihHC4826rdYIiKn86kALyot4mTxSadLERGpF3wmwE8/IqGIiPhQgOuAViIiZ/K5ANchZUVE3HwmwHVEQhGRM/lMgKuFIiJyJp8J8JYhLQHIzm/cO/qLiJTzmQAPcYXQPLg5aSdqdgB0EZGGwmcCHCCuaZwCXETEw6cCvF3Tduw/vt/pMkRE6gWfCvC4iDjSjmsGLiICPhbg7Zq242jhUfKK8pwuRUTEcT4V4HFN4wA0CxcRwccCvF1EOwB9kCkigo8FePkMXB9kioj4WIC3Dm+NwaiFIiKCjwW4y99F6/DW7D+hGbiIiE8FOLj3RNEMXETEBwM8rmmceuAiIlQhwI0xM40xWcaYzacta2GMWWqM2en52bxuy/yvdhHtSD+R7q3NiYjUW1WZgb8NjDpr2ePAMmttF2CZ57pXNA1qSkFJAcWlxd7apIhIvXTRALfWrgTOPgj3TcAsz+VZwNharqtSYYFhAOQX53trkyIi9VJ1e+Ax1toMAM/P6MpWNMbca4xJMsYkZWfX/FjeoYGhAOQXKcBFpHGr8w8xrbUzrLWJ1trEqKioGt9eqMsT4JqBi0gjV90AzzTGtALw/MyqvZIurLyFogNaiUhjV90A/wSY7Lk8GZhfO+VcnFooIiJuVdmN8H1gLdDNGJNujLkb+BNwrTFmJ3Ct57pXqIUiIuIWcLEVrLWTKvnV8FqupUrUQhERcfO5b2KqhSIi4uZ7Aa4WiogI4IMBrhaKiIibzwV4E1cTQC0UERGfC3A/40eIK0QtFBFp9HwuwMHdRlELRUQaO58M8FBXqGbgItLo+WSAhwWGqQcuIo2eTwZ4aGCoWigi0uj5ZoCrhSIi4psBrhaKiIiPBrhaKCIivhrgaqGIiPhmgKuFIiLiowEe6nK3UKy1TpciIuIY3wzwwFAslsKSQqdLERFxjE8GePkRCdUHF5HGzCcDvPyY4NoTRUQaM98McJ2VR0TENwNcLRQRER8NcLVQRER8NMArZuBqoYhII+aTAV7RA1cLRUQasRoFuDHml8aYVGPMZmPM+8aY4Noq7ELUQhERqUGAG2PaAP8DJFprewP+wMTaKuxC1EIREal5CyUAaGKMCQBCgIM1L+niylsomoGLSGNW7QC31h4Angf2AxnAcWvtkrPXM8bca4xJMsYkZWdnV7/S0wT6BxIbFsvGrI21cnsiIr6oJi2U5sBNQAegNRBqjPnp2etZa2dYaxOttYlRUVHVr/Qs13e5nkW7FlFUWlRrtyki4ktq0kK5Bthrrc221hYD84Af1U5ZF3dDtxs4ceoEq/at8tYmRUTqlZoE+H5goDEmxBhjgOHA1top6+Ku6XgNwQHBfLL9E29tUkSkXqlJD3w98DGQDGzy3NaMWqrrokJcIQzvMJxPd3yq44KLSKNUo71QrLVPW2u7W2t7W2tvt9aeqq3CquLq+KvZe2wvRwuPenOzIiL1gk9+E7Nc6/DWAGTmZTpciYiI9/l0gMeGxQKQma8AF5HGx6cDPCYsBoBDeYccrkRExPt8OsDLZ+AKcBFpjHw6wJsHN8fl51IPXEQaJZ8OcGMMMWExHMrXDFxEGh+fDnBwt1HUQhGRxsjnAzwmNEYtFBFplHw+wDUDF5HGqkEEeFZ+FmW2zOlSRES8yucDPCY0hlJbSmZeJhsObXC6HBERr/H5AC/fF/yxLx6j34x+ZORmOFyRiIh3+HyAl38b8/1N71Nmy9h62GtHtBURcZTPB3j5DLzUlgKw/fB2J8sREfGaAKcLqKnyAPc3/rj8Xew4ssPhikREvMPnAzw8MJwQVwhD44dyMPcgO3IU4CLSOPh8C8UYw+ybZ/PiyBfpGtlVLRQRaTR8PsABxnYfS9fIrnSL7MbeY3t1pnoRaRQaRICX6xrZlTJbxp6je5wuRUSkzjW4AAf0QaaINAoNMsDVBxeRxqBBBXiz4GZEh0az5fAWp0sREalzDSrAAa5oewWr9q1yugwRkTpXowA3xjQzxnxsjNlmjNlqjLmitgqrrmEdhrH76G72HdvndCkiInWqpjPwvwCLrLXdgb6A4wciGdZhGADL9y53uBIRkbpV7QA3xkQAg4E3Aay1RdbaY7VVWHX1iupFdGg0y79XgItIw1aTGXhHIBt4yxjznTHmDWNM6NkrGWPuNcYkGWOSsrOza7C5qjHGMKzDMJbvXY61ltKyUhbsXIC1ts63LSLiTTUJ8ACgH/B3a+2lQD7w+NkrWWtnWGsTrbWJUVFRNdhc1Q2LH8bB3INsyd7Ce5veY8x7Y1i9f7VXti0i4i01CfB0IN1au95z/WPcge64MV3HAPDvbf9m/vb5AGzM3OhkSSIita7aRyO01h4yxqQZY7pZa7cDw4F6sQN26/DWDGw7kDmpcyq+Vp+anepwVSIitaume6FMBWYbYzYCCcAfa15S7bi5+81sytpEfnE+Qf5BbMmuF68tIiK1pkYBbq1N8fS3+1hrx1prj9ZWYTU1rsc4AEJdodza61bNwEWkwWlw38Qs17lFZ65oewXjeozjslaXcfjkYbLys5wuS0Sk1vj8GXkuZMXkFfgZP778/ksAUrNSie4Q7WxRIiK1pMHOwAGCAoJw+bvoFd0L0AeZItKwNOgAL9cqrBXNgpuRmqUAF5GGo1EEuDGGXlG9KmbgJWUlFJcWO1yViEjNNIoAByoC3FrLPZ/ew+j3RjtdkohIjTSeAI/uRU5BDpn5mSzbs4wvv/+SguICp8sSEam2xhPgUe4PMlfuW0naiTRKykr4NuNbh6sSEam+RhPgPaN6AvDuxncrlq1NWwtAflE+Dy94mF05uxypTUSkOhpNgMeGxdI8uDkLdy0EIDo0mrXp7gD/3Ve/49VvXuXzHZ87WaKIyA/SaALcGEOv6F6UlJUQ1zSOEZ1GsDZ9Ld9lfMcLa18A4FDeIYerFBGpukYT4PDfPni/Vv24ou0VHMo7xNBZQ2kZ0pLIJpFk5mc6XKGISNU1ygC/NPZSBrcfjMHQM6onq+5cRftm7TUDFxGf0qCPhXK2fq3c55sY0GYAvaN7s2PqDuKbxRPgF0BsWCwZuRkOVygiUnWNagb+o3Y/Yt3d6xjRaQTgPmJhgJ/7NSw2NFYtFBHxKY0qwI0xDGg7AGPMOb+LCYshMy+TMlvmQGUiIj9cowrwC4kNi6XUlpJTkON0KSIiVaIA94gNiwXgYO5BHln8CJsyNzlckYjIhSnAPWJCYwD46vuveHHdi8zaMMvhikRELkwB7lE+A/98p/vbmN8d+s7JckRELkoB7lEe4Cu+XwFAyqEUrLVOliQickEKcI+IoAiC/IMoKi0CIKcgh7QTaQ5XJSJSOQW4hzGmYhY+oM0AwD0LFxGpr2oc4MYYf2PMd8aYz2qjICeVB/iUhCkYjAJcROq12piB/xzYWgu347iYMPeeKEPaD6FrZFd9kCki9VqNAtwY0xYYA7xRO+U4Ky4ijqZBTeka2ZWE2AS+y1CAi0j9VdMZ+EvAY0CD+P75/w75X1bduQp/P38ua3UZ+47vIzs/2+myRETOq9oBboy5Hsiy1l7wxJLGmHuNMUnGmKTs7PodhlGhUVwScwkAA9sOBGBd+jonSxIRqVRNZuCDgBuNMd8DHwDDjDHvnr2StXaGtTbRWpsYFRVVg815V2LrRAL8AipOuyYiUt9UO8CttU9Ya9taa+OBicBya+1Pa60yhzVxNSEhNkEzcBGpt7Qf+AUMbDOQrw98TUlZidOliIico1YC3Fr7pbX2+tq4rfrkinZXkF+cT2pWqtOliIicQzPwCyj/IFN9cBGpjxTgF9ChWQfaRrRl2d5lTpciInIOBfgFGGMY1WkUS3Yvobi0mLlb5rLt8DanyxIRARTgFzW6y2hOnDrBzO9mcutHt3L7v27XYWZFpF5QgF/E8I7DCfAL4H8W/Q8WS9LBJD7Z/onTZYmIKMAvJiIogivjrqSotIhfDPgFnVt05qkVT+ns9SLiOAV4FUzoOYFmwc34zZW/4XdDf8emrE18lPqR02WJSCNnvNnPTUxMtElJSV7bXm2x1lJYUkgTVxNKy0rp+1pfSspK2PzgZgL8ApwuT0QaOGPMt9baxLOXawZeBcYYmriaAODv58+0q6ex/ch23tv0nsOViUhjpgCvhnHdx9Ezqiczvp3hdCki0ogpwKvBGMNtvW5jTdoaMnIznC5HRBopBXg1je8xHovl39v+7XQpItJI6RO4auoZ1ZNukd34IPUDQlwh9IruRWLrcz5jEBGpM5qBV5MxhvE9xrNy30qmzJ/C/Z/d73RJItLIaAZeAw/2f5Bjhcc4UXSCdze+S2ZeZsWZ7UVE6ppm4DXQJqINr455lV8O/CUAi3cvdrgiEWlMFOC1ICE2gdiwWBbsXOB0KSLSiCjAa4Gf8WNUZ/dhZ5fvXU7a8TSnSxKRRkABXktu6HoDRwuPMvyd4XT5axeeWv4UpWWlTpclIg2YPsSsJWO7j2XF5BWU2TLeSH6D6aum06lFJ6YkTHG6NBFpoHQwqzpgraXX33rRLLgZa+5e43Q5IuLjdDArLzLG8LN+P2Nt+lo2Z212uhwRaaAU4HXkjr53EOgfyEvrXtIp2ESkTijA60jLkJbcmXAnb373JtfNvo7MvEynSxKRBqbaAW6MaWeMWWGM2WqMSTXG/Lw2C2sI/jbmb7xy3Sus3LeSwW8PJv1EutMliUgDUpMZeAnwK2ttD2Ag8JAxpmftlNUw+Bk/Hrr8IZbcvoSM3Ayum33dGbsWfvn9l+SeynWwQhHxZdUOcGtthrU22XM5F9gKtKmtwhqSK+Ou5M0b32Rz1mbmpM4BYN+xfVw962p+vkhvXESkemqlB26MiQcuBdaf53f3GmOSjDFJ2dnZtbE5nzS+53guib6EaV9No7SslEW7FgEwa8MstmZvdbg6EfFFNQ5wY0wYMBf4hbX2xNm/t9bOsNYmWmsTo6Kiaro5n+Vn/Hh6yNNsP7KdDzZ/wKLdi4gNiyXEFcJTK546Y92vvv9K/XIRuagaBbgxxoU7vGdba+fVTkkN17ge4+gT04dpK6exbM8ybux6I7++4tfM3TqXL7//EoD9x/dzzT+v4ekVTztbrIjUezXZC8UAbwJbrbUv1F5JDVf5LHzHkR3kFuVyXZfreHTQo8Q3i+ehBQ9RXFrMS+teoqSshK8Pfl3xdyVlJaxJc3+jc23aWsKfCWf74e1O3Q0RqSdqMgMfBNwODDPGpHj+ja6luhqssd3H0iemDwF+AQzrMIwQVwgvj3qZLdlbmPDxBF5Pfp1A/0C2ZG8hvygfgJfXv8ygmYNYsXcFrye/Tl5RHp/t+MzheyIiTqvJXiirrbXGWtvHWpvg+acDYl+En/Hjn+P+ybvj3iUiKAKAG7rdwB+G/YHFuxaTV5TH00OepsyWkZyRTGlZKa98/QoAz615jrlb5wKwbO8ywN1yEZHGSd/EdECfmD7c1vu2M5b9v6v+Hzun7uTLyV9y96V3A/DNwW/4fOfn7D22l74xfVm4ayEnTp2gV1QvVu5byUepH9H+pfbM+HaGE3dDRBymAK9H2kS0YUj8EGLCYmgX0Y71B9bz/JrnaRvRlrkT5uJn/IgNi+XpIU+TX5zPfZ/dB8ATy57gyMkjDlcvIt6mAK+n+rfpz4epH7Jq/yqeGvwUnVp04vdX/54/DvsjwzsOx2A4WniUxwc9zvHC4+fsigiw9+heHQ1RpAFTgNdT/Vv3B+CXA3/JvZfdC7jbLHdeeictmrRgQNsBdIvsxvRh07mj7x28s+EdCksKASguLeau+XfR+a+dGTRzEEWlRY7dDxGpOwrweuqefvfw1k1v8fyI58/7+7kT5rJi8gr8/fy5rddt5Bfns3T3UgDmbZ3HWylvMTR+KCdOnWBt2lpvli4iXqIAr6ciQyKZkjAFP3P+h6h1eGtahbcC4OoOV9M0qCn/2vYvAF7++mU6Ne/Ex7d+jL/xZ+mepee9jaW7l+oEzCI+TAHeAAT6BzKm6xg+2f4J69PXsyZtDVMvn0rzJs0Z0HbAeQN89sbZjHh3BGPeG0NRaRHr0ted9+v7r3z9Cq9+/ao37oaI/EAK8Abi5u43c6TgCFe8eQVhgWEVJ1O+tuO1JB1MIqcgB2stL6x9gUlzJ3HXJ3fRNbIrm7I2cc0713DFm1fw47k/PuM2rbX8cdUf+e1Xv6XMljlwr0TkQhTgDcT1Xa/nsR89xpNXPckXt39B0+CmgDvAy2wZi3YtYuGuhfxqya9Ym7aWaztey7q71zGp9yRW7V9Fx+YdWbV/FZsyN1Xc5t5je8nIy+DwycMkHWz4J6MW8TUBThcgtSMoIIg/X/vnc5aX763yyOJHiAyJpFPzTmx9aCsufxcAM26YwU/7/JTL21xOuxfb8fekv/O3MX8DYPX+1RW3s2DnAi5vc7l37oyIVIlm4A1cgF8AcyfMJa8ojy3ZW/j91b+vCG+AsMAwRncZTcuQlkzsPZF3NrzDE188wdbsrazev5pmwc0Y0GYA87fP57aPb2PkuyPJK8o7ZzsFxQVMXTCV5Ixkb949kUbNePOM6YmJiTYpSW/FnbBw50IW717MCyNfqHTPll05u/jJvJ+QnJFMy5CWBAcE0yuqFwPbDqz4opCf8ePq+Kv5dNKnNHE1qfjbez65hze+e4M+MX1IvjcZfz9/r9wvkcbAGPOttTbx7OWagTcS13W5jpdGvVRpeAN0btGZ9T9bT9I97g89vz/2PVfGXcktPW8hJjSG1294nbdueotle5fRb0Y/Fu9aTNrxNJ744gne+O4NBrcfzMbMjbyd8rb37phII6YZuJzXC2tf4FdLfsXXP/ua/m36Y63FfQh49/7jd86/kwO5ByrWn9R7Eu+Me4chbw9hTdoawgPDCQsMo2tkV2aNnUX7Zu2duisiPq+yGbgCXM7LWsvuo7vp3KLzeX+feyqXlftWsv3Idq7peA19YvoAkH4inTeS3+B44XFyi3KZu3Uuoa5Qlt6+lB5RPVi8azGLdi2isKSQ50Y8R1hgmDfvlohPUoCLIzZmbmTEP0cQ4grhj8P/yE/m/YTggGBOFp/kyaueZPqw6QD8c8M/CfQPPOcwuwAHcw8y/sPx3Nj1Rh7o/wCr9q0iKjSKhNgEggOCz7vdJbuXEBsWW/HCIuLLFODimPXp6xn89mCKSovoHd2btXev5f7P7ufjLR+z9aGtLN2zlPs+u48AvwBWTlnJzO9mklOYw6yxswhxhTDy3ZEs37ucMluGwWBxP2c7Nu/Ixvs3EhoYesb2Vu9fzZC3hxAcEMxHt35E/9b9aRnSsqIFJOJrFODiqHc3vsuz/3mWebfNo3OLzqSfSKfbK904WXwSgJGdRpKancqhvEOUlJXgZ/zo16ofrcJa8emOT/nH9f8gskkk3xz8hhGdRrDjyA4e+PwB/nrdX3n48ocpLCkkOCCYwycPc9mMywjwCyAsMIyNmRsBGNV5FJ9N+qzSvWOKS4spLCkkPCi8YllRaREBfgEX/OBXxBsU4FLv/Gf/f1i2dxlhgWHcn3g/69PXM/7D8fxh2B+IDYtlyvwpRARFMKHnBJ4f8fwZM2hrLYNmDiIjL4Or4q5iTuocJvSawBd7vuDIySOsvms1XSO78t6m99h5ZCcvrX+JpwY/RecWnUk5lML+4/vZmLmRXtG9ePumtxk7ZyxfH/ia+y+7nxu73cjq/av53Ve/o3vL7jw95GnG9Rh3TpDnnsolLDCsSjP74tJiluxewrAOwyp2v9yds5u9x/ZyTcdrandgpcFRgItPKLNlFUF5+uXz+fe2fzNuzjgARncZzfK9y+nesjtv3vgm/Vr1q1jPWsvEuRP5MPVDAEJcIbSNaEuXFl1YsHMBUaFRZOVnMbLTSJbuWVpx3Jcbu93I9sPb2X5kO31i+pAQm0BEYAT3Jd7H82ueZ9aGWYQHhtMmog3tItrxxJVPVLxAjOg0gqCAIHJP5bJk9xKe/vJpUrNTGdZhGJ9O+pT0E+lc9dZVHD55mA33b6BFkxYkZyQzpsuY874glP8/VRuocVKAS4NTZst48PMHuTLuSn7a56cUlxYT4Bdw3pA7ceoEr3/7OlfGXcnlbS6vWOe1pNd44PMHeHzQ4zxzzTNk5mWSnJFME1cThsYPpbSslA82f8D/rf0/cgpyyMzPrDhxxoOJD+Jn/DiUf4j16etJO/HfQ/P2ienDpbGX8sHmDzhVeoq4pnFM6j2JZ//zLK3CW5FflE+gfyDFZcVcEn0JB3MPsvvobkZ3Gc3z1z5Pj6geAOQX5fPepveYtnIaOQU5dGzekcTWieQV5bHzyE5m3jTzjBerFXtX8Piyx3nsR48xvuf4SseutKxUX7byIQpwkUoczD1Iq7BWVZrdHso7xMvrX2ZQu0GM6TqmYnlBcQGvJb1GYUkh7Zq249dLfk1uUS6T+05mQq8JDGo3CJe/i3lb5/HB5g8I9A/ksUGPsXzvcn65+JeEuEKYevlU/rL+LxSWFNK+aXtCA0PZeWQnxWXFDGw7kIFtBrIjZwffHPiGJq4mFJYU4mf8mNhrIm9+9yatw1uzM2cnAX4BFJcWc3vf2ykuLebajtfy40t+TFBAEACLdi3i9n/dzvge4/n7mL9jjOGbA9+waNcixnYfyyUxl/yg8SsoLiDpYBJ9Y/sSERRxxu+OFR4j1BXK3mN7ue3j2xjVaRTPXPNMxVhm5WdxSfQlemdxEQpwES86VXKKkrKSc/aQOVtRaRGPLH6EW3rewtD4oWTkZvDxlo9Zk76GUyWn6NyiM9d1vo6h8UPPCbnUrFQGzRzEiVMnuKXnLe4Xj4h2/Hbob5m6cCoLdy0k1BVKRl4Gfsav4stVB3MPEh0aTWZ+JhN7T6S4tJh5W+dV7N3TvWV3+rfuT/Pg5uQV5ZF1MouSshKstRSWFJKanUrToKZMSZhCanYqn+34jLyiPJoHN+exQY/x6I8eJTkjmadWPMXSPUtpGtSUMltGfnE+JWUlPDLwERbvXkxqdioAwzoM4+cDfk7H5h2JCIogNiyWQP/Ac8bz8MnDlNkyymwZO47sqNhVtFd0L3JP5TJ/+3w2Z21mSPsh3NLzFq6Mu3+CGgEAAAiZSURBVBJjDLtydrE5azNXx19N2ok0Fu1axMTeE/E3/szeNJs7+t5BdGj0GdvLKcihWXAzwH0gt7imcTXaJXVz1mZ6R/eu9t/XSYAbY0YBfwH8gTestX+60PoKcJHatf3wdopKiyqdNVtr+WLPF3y17ytyT+WSV5RHTFgMT171JE8se4K/fv1XWoe35se9f8xDlz/E/G3z+WLvF6QcSiH3VC4hrhBiwmJw+bkwxuDyc9EtshvbjmxjTdoaokKiGNd9HMM6DGP2ptl8uuNTBrYdSHJGMpFNIpncdzIH8w5y+ORhXhz5Ig8veJile5bSNbIr9192PxbL9JXTOVp4tKLmEFcIV8VdRcuQlhw/dZxth7ex5+iec45J7/JzUVxWXHG9WXAzLo29lHXp6ygoKagI5az8LACCA4Ir2l9hgWEYDLlFubSLaMd9l91HSmYKXVt0ZdfRXXyY+mHF7qcLdy0EYEj7ITwz/JmKmib1nsTOnJ0s2LmA8MBwTpWe4mjBUcICw2gW3IyWIS0ZGj+Umd/NZPqq6cydMJebe9xcrce51gPcGOMP7ACuBdKBb4BJ1totlf2NAlykfikoLjjjoGQ/RNrxNFqFtyLA779Hpf5H0j+YunAqQ+OH8v7494kMiTzjb3JP5bJ873JGdxldcVTMvKI8NmdtZt+xfeQW5ZJyKIVV+1eRV5RHqCuU7i27071ld9qEt6nYrTM6NJqrO1zN8cLj7D66mxBXCD2jehIcEEx+UT4fb/mYFd+vIMg/iF7RvegV1Yv52+fTokkLbuh6A9NXTcday50JdzJ14VTSTqQR1zSOAycOEBQQxOS+k5m3dR45BTk8e+2zlJaV8tya58jMz6y4L/7Gn1Jbip/xq/iOQnhQOPlF+ZTa0jPu910Jd/HK6FeqPdZ1EeBXAL+11o70XH8CwFr7TGV/owAXafiOFhylaXBTn9l/vqC4gGOFx2gV3oq8ojystYQHhZN7KpcjBUeIbxYPuF983kp5i1ZhregZ1ZNZG2YR1zSOOxPuxM/44fJ3EeAXgLWW/OJ80o672zVxTeMu+IFyVdRFgN8CjLLW/sxz/XZggLX24bPWuxe4FyAuLu6yffv2VWt7IiKNVV0cTvZ8Hxuf82pgrZ1hrU201iZGRUXVYHMiInK6mgR4OtDutOttgYM1K0dERKqqJgH+DdDFGNPBGBMITAQ+qZ2yRETkYqp9UmNrbYkx5mFgMe7dCGdaa1NrrTIREbmgGp2V3lq7AFhQS7WIiMgP4Bv7+YiIyDkU4CIiPkoBLiLio7x6MCtjTDZQ3W/ytAQO12I5taW+1gX1tzbV9cPU17qg/tbW0Opqb60954s0Xg3wmjDGJJ3vm0hOq691Qf2tTXX9MPW1Lqi/tTWWutRCERHxUQpwEREf5UsBPsPpAipRX+uC+lub6vph6mtdUH9raxR1+UwPXEREzuRLM3ARETmNAlxExEf5RIAbY0YZY7YbY3YZYx53sI52xpgVxpitxphUY8zPPct/a4w5YIxJ8fwb7UBt3xtjNnm2n+RZ1sIYs9QYs9Pzs7mXa+p22pikGGNOGGN+4dR4GWNmGmOyjDGbT1t23jEybi97nnMbjTH9vFzXc8aYbZ5t/8sY08yzPN4YU3Da2L3m5boqfeyMMU94xmu7MWakl+uac1pN3xtjUjzLvTleleVD3T3HrLX1+h/uIx3uBjoCgcAGoKdDtbQC+nkuh+M+J2hP4LfArx0ep++BlmctexZ43HP5ceDPDj+Oh4D2To0XMBjoB2y+2BgBo4GFuE9cMhBY7+W6RgABnst/Pq2u+NPXc2C8zvvYef4fbACCgA6e/7P+3qrrrN//H/C/DoxXZflQZ88xX5iBXw7sstbusdYWAR8ANzlRiLU2w1qb7LmcC2wF2jhRSxXdBMzyXJ4FjHWwluHAbmutY+fUs9auBHLOWlzZGN0EvGPd1gHNjDGtvFWXtXaJtbbEc3Ud7hOmeFUl41WZm4APrLWnrLV7gV24/+96tS5jjAEmAO/XxbYv5AL5UGfPMV8I8DZA2mnX06kHoWmMiQcuBdZ7Fj3seRs009utCg8LLDHGfGvc5yEFiLHWZoD7yQVEO1BXuYmc+Z/K6fEqV9kY1afn3V24Z2rlOhhjvjPGfGWMucqBes732NWX8boKyLTW7jxtmdfH66x8qLPnmC8EeJXOvelNxpgwYC7wC2vtCeDvQCcgAcjA/RbO2wZZa/sB1wEPGWMGO1DDeRn3GZtuBD7yLKoP43Ux9eJ5Z4x5EigBZnsWZQBx1tpLgUeA94wxEV4sqbLHrl6MFzCJMycKXh+v8+RDpaueZ9kPGjNfCPB6de5NY4wL94Mz21o7D8Bam2mtLbXWlgGvU0dvHS/EWnvQ8zML+Jenhszyt2Sen1nersvjOiDZWpvpqdHx8TpNZWPk+PPOGDMZuB74ifU0TT0tiiOey9/i7jV39VZNF3js6sN4BQA3A3PKl3l7vM6XD9Thc8wXArzenHvT0197E9hqrX3htOWn963GAZvP/ts6rivUGBNefhn3B2CbcY/TZM9qk4H53qzrNGfMipwer7NUNkafAHd49hQYCBwvfxvsDcaYUcBvgButtSdPWx5ljPH3XO4IdAH2eLGuyh67T4CJxpggY0wHT11fe6suj2uAbdba9PIF3hyvyvKBunyOeePT2Vr4dHc07k90dwNPOljHlbjf4mwEUjz/RgP/BDZ5ln8CtPJyXR1x7wGwAUgtHyMgElgG7PT8bOHAmIUAR4Cmpy1zZLxwv4hkAMW4Zz93VzZGuN/evup5zm0CEr1c1y7c/dHy59lrnnXHex7jDUAycIOX66r0sQOe9IzXduA6b9blWf42cP9Z63pzvCrLhzp7jumr9CIiPsoXWigiInIeCnARER+lABcR8VEKcBERH6UAFxHxUQpwEREfpQAXEfFR/x9+kaImk1L1JgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, label='losses', color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of in test set:  0.794\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_, prediction = model(A.to(device),features.cuda()).max(dim=1)\n",
    "acc = acc_calc(label_list[test_mask].to(device),prediction[test_mask].to(device))\n",
    "print(\"accuracy of in test set: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe still have a gap between our model and the model assigned in paper--code can be see in(https://github.com/tkipf/gcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
